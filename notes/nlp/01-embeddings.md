***
# Motivaci칩n

- Una vez tokenizado nuestro corpus, utilizando alguna de las t칠cnicas de tokenizaci칩n como las que describimos [anteriormente](00-intro.md), debemos convertir nuestro vocabulario (i.e., tokens) a una representaci칩n num칠rica.
- Una posible soluci칩n es utilizar el m칠todo de *One-Hot Encoding*. Este m칠todo consiste en generar, para cada token, un vector de largo $|V|$ (i.e., tama침o de nuestro vocabulario) que tenga un $1$ en la posici칩n/칤ndice correspondiente a ese token, y $0$ en el resto de las posiciones.
	- Esto m칠todo tiene varios problemas:
		- **Dimensi칩n de los vectores.** La dimensi칩n de nuestros vectores aumenta con el tama침o de nuestro vocabulario. Si el tama침o de nuestro vocabulario es muy grande, entonces vamos a tener vectores con muchas dimensiones (demasiadas!).
		- **Matem치ticamente inc칩modos.** Sin vectores ralos (i.e., tienen muchos ceros). Esto los hace muy inc칩modos para realizar operaciones matem치ticas.
		- **Ortogonalidad.** Todos los vectores son ortogonales. Esto quiere decir que no aportan informaci칩n relevante sobre las palabras.
- Nos gustar칤a que nuestras representaciones vectoriales incluyan informaci칩n sint치ctica del token. Otra informaci칩n importante que los vectores deber칤an incluir es informaci칩n de contexto.
	- Seg칰n la *sem치ntica distribucional*, el significado de una palabra depende de como se utiliza y en qu칠 contexto.

# Word Embeddings

- Los embeddings son representaciones vectoriales, de baja dimensionalidad, que contienen informaci칩n sem치ntica relevante sobre nuestros tokens.
- Lo bueno es que estas representaciones se encuentran en un mismo espacio vectorial, y conservan propiedades sem치nticas y geom칠tricas que nos permiten que tokens similares tengan embeddings cercanos en el espacio.

## LSA

- Consiste en aplicar una reducci칩n de dimensionalidad, utilizando PCA o SVD, sobre una matriz de frecuencia. De esta manera podemos obtener mejores representaciones vectoriales.
- La matriz de frecuencia la construimos con todos los documentos de nuestro corpus. Lo que hacemos es contar cuantas veces aparece el token en cada documento, generando as칤 un vector de frecuencias para cada token.
	- La idea es que los tokens que tienen alguna relaci칩n sem치ntica van a tener vectores similares porque aparecen con mayor frecuencia en los mismos documentos.
	- Algunos problemas con estos vectores:
		- Son ralos.
		- La dimensionalidad depende de la cantidad de documentos en nuestro corpus.
		- El tiempo de ejecuci칩n crece m치s que linealmente cuando agregamos m치s documentos a nuestro corpus.

## Similaridad Coseno

- Podemos usar la similaridad coseno para entender si dos tokens son similares o no. $$cossim(\bar{v}_{1},\bar{v}_{2})=\text{cos}(\text{angle})=\frac{\bar{v}_{1}\cdot\bar{v}_{2}}{|\bar{v}_{1}|\cdot|\bar{v}_{2}|}\in[-1,1]$$
- La idea es la siguiente:
	- Cuando $cossim(\bar{v}_{1},\bar{v}_{2})=1$, entonces el 치ngulo entre ambos vectores es de $0$ grados, es decir, el vector es el mismo.
	- Cuando $cossim(\bar{v}_{1},\bar{v}_{2})=0$, entonces el 치ngulo entre ambos vectores es de $90$ grados, es decir, los vectores son ortogonales. 
		- Los vectores no comparten informaci칩n relevante entre s칤, y no hay ninguna relaci칩n sem치ntica entre ellos.
	- Cuando $cossim(\bar{v}_{1},\bar{v}_{2})=-1$, entonces el 치ngulo entre ambos vectores es de $180$ grados, es decir, los vectores son opuestos. 
		- Los vectores est치n relacionados conceptualmente pero son opuestos en cuanto a la sem치ntica.

# Word2Vec

- Los autores introducen un m칠todo muy eficiente para generar embeddings. Estos embeddings se pueden entrenar sobre corpus muy grandes y mejoran mucho la performance en tareas downstream.
- La idea es entrenar un modelo sobre una tarea auxiliar (predecir una o m치s palabras), y utilizar las representaciones vectoriales que aprendi칩 el modelo durante el entrenamiento como embeddings.
	- Muy parecido a lo que ocurre con los autoencoders, en donde no nos interesa la tarea auxiliar, solo nos interesa las representaciones que aprende el modelo cuando lo entrenamos.
	- A diferencia de otros m칠todos, que s칩lo utilizan las 칰ltimas $N$ palabras de la oraci칩n, Word2Vec utiliza todo el contexto, es decir, utiliza las palabras antes y despu칠s de la palabra que queremos predecir. Esto nos permite generar mejores embeddings.
- Hay dos posibles tareas auxiliares (dependiendo de que tarea elijamos, la arquitectura de nuestro modelo va a cambiar) para entrenar nuestro modelo.
	- **Skip-gram.** Dada una palabra, queremos predecir las palabras del contexto (ventana m칩vil que definimos nosotros).
	- **Continuous Bag of Words (CBoW).** Dadas las palabras del contexto, predecir la palabra del centro.
<figure>
	<img src='attachments/Word2Vec.png' style="display: block; margin: 0 auto;"/>
</figure>

- Para construir el embedding de la palabra *padre*, vamos a mirar las palabras (o tokens) que la acompa침a con mayor frecuencia. De esa manera, si la palabra *padre* suele estar acompa침ada por la palabra *hijo*, entonces ambas palabras van a tener embeddings similares.

## Skip-gram

- En este caso, nuestra tarea auxiliar consiste en predecir el contexto a partir de una 칰nica palabra.
- La salida de nuestra red neuronal son vectores de dimensi칩n $|V|$ (tama침o del vocabulario) con la probabilidad para cada uno de los tokens en nuestro vocabulario de pertenecer al contexto.
<figure>
	<img src='attachments/skipgram.png' style="display: block; margin: 0 auto;"/>
</figure>

- La matriz de pesos, $W$, que aprende el modelo durante el entrenamiento es una matriz de dimensi칩n $V\times d$, donde cada fila representa el embedding de uno de los tokens del vocabulario.
- La dimensi칩n $d$ de los embeddings determina cuanta informaci칩n vamos a incluir.
	- Si $d>>0$, entonces los embeddings van a estar muy separados.
	- Si $d<<0$, entonces los embeddings van a estar muy cerca.

## CBoW

- En este caso, nuestra tarea auxiliar consiste en predecir una palabra a partir del contexto.
- La salida del modelo es un 칰nico vector, de dimensi칩n $|V|$, con la probabilidad que tiene cada token en el vocabulario de ser la palabra que buscamos predecir.
<figure>
	<img src='attachments/cbow.png' style="display: block; margin: 0 auto;"/>
</figure>

## Negative Sampling

- Si el tama침o de nuestro vocabulario es muy grande, entonces actualizar los pesos de todos los $|V|$ embeddings en cada paso del entrenamiento es computacionalmente muy costoso e ineficiente.
- La soluci칩n que plantean los autores se conoce como *negative sampling*. La idea es actualizar los pesos solo de un grupo acotado de palabras en nuestro vocabulario, haciendo m치s eficiente el entrenamiento del modelo.
	1. Primero tenemos que extraer de nuestro corpus un par de palabras, donde la primera palabra es la que queremos predecir y la segunda palabra es el contexto. Esto se conoce como *positive example* (porque ocurren al mismo tiempo).
		- Un ejemplo ser칤a $(\text{perro}, \text{amigo})$.
	2. Seleccionamos aleatoriamente $k$ palabras que no aparecen en la ventana de contexto, y con ella generamos $k$ nuevos pares. Estos son los *negative examples* que vamos a utilizar para el entrenamiento.
		- Un ejemplo ser칤a $(\text{perro}, \text{telescopio})$.
	3. Para cada par de palabras, entrenamos el modelo para que nos devuelve $1$ si es el *positive example* y $0$ si es un *negative sample*. Es decir, entrenamos a nuestro modelo para que nos diga si un par de palabras es real, como $(\text{perro}, \text{amigo})$, o es ficticio, como $(\text{perro}, \text{telescopio})$. Para eso utilizamos una funci칩n de p칠rdida log칤stica.
	4. Una vez calculada la funci칩n de p칠rdida, computamos los gradientes y actualizamos los pesos de la palabra que buscamos predecir (en nuestro ejemplo, $\text{perro}$), el contexto (en nuestro ejemplo, $\text{amigo}$) y las palabras seleccionadas aleatoriamente (en nuestro ejemplo, $\text{telescopio}$).

***
## 游닄 Recursos
- Alammar, Jay._The Illustrated Word2vec_. Jay Alammar, 19 Nov. 2017,[https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/).