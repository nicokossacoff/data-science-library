{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, precision_score, confusion_matrix\n",
    "from numpy.random import multivariate_normal\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "\n",
    "Generar una muestra de vectores aleatorios de tamaño 20 con distrbución $N_{2}(\\mu,\\Sigma)$, con $\\mu=(0,0)$ y $\\Sigma=Id$. ¿Quiénes son las componentes principales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0,0])\n",
    "cov = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "sample = multivariate_normal(mean=mean, cov=cov, size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, como la matriz de covarianza es igual a la matriz identidad, los autovectores son iguales a las columnas de la covarianza. Dicho esto, los componentes principales son iguales a las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print('Autovectores:\\n')\n",
    "print('Primer autovector:', eigenvectors[:, 0])\n",
    "print('Segundo autovector:', eigenvectors[:, 1])\n",
    "\n",
    "print('\\nAutovalores:\\n')\n",
    "print('Primer autovalor:', eigenvalues[0])\n",
    "print('Segundo autovalor:', eigenvalues[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "\n",
    "Generar una muestra como la anterior, pero utilizar $\\Sigma = \\begin{pmatrix} 2 & 1.2 \\\\ 1.2 & 1 \\end{pmatrix}$. Calcular los autovectores de $\\Sigma$ y dar las componentes muestrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0,0])\n",
    "cov = np.array([[2, 1.2], [1.2, 1]])\n",
    "\n",
    "sample = multivariate_normal(mean=mean, cov=cov, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print('Autovectores:\\n')\n",
    "print('Primer autovector:', eigenvectors[:, 0])\n",
    "print('Segundo autovector:', eigenvectors[:, 1])\n",
    "\n",
    "print('\\nAutovalores:\\n')\n",
    "print('Primer autovalor:', eigenvalues[0])\n",
    "print('Segundo autovalor:', eigenvalues[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar los datos y los autovectores. Añadir al mismo gráfico los datos proyectados en el primer componente y proyectados a las segunda componente (sugerencia: usar la funcion eigen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=sample[:, 0],\n",
    "    y=sample[:, 1],\n",
    "    size=[1] * sample.shape[0]\n",
    ")\n",
    "\n",
    "# Add another scatter plot with cross markers\n",
    "figure.add_scatter(\n",
    "    x=eigenvectors[:, 0],\n",
    "    y=eigenvectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=25, color='black'),\n",
    "    name='Autovectores'\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "\n",
    "*Generar una muestra aleatoria de tamaño $50$ con matriz de covarianza diagonal tal que el primer autovector se corresponda con el $90\\%$ de la variabilidad de la muestra medida como la traza de $\\Sigma$. Una vez logrado el primer objetivo poner un outlier en la dirección del segundo autovector y calcular las PCA.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0,0])\n",
    "cov = np.array([[0.9, 0.1], [0.1, 0.1]])\n",
    "\n",
    "sample = multivariate_normal(mean=mean, cov=cov, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "print('Autovectores:\\n')\n",
    "print('Primer autovector:', eigenvectors[:, 0])\n",
    "print('Segundo autovector:', eigenvectors[:, 1])\n",
    "\n",
    "print('\\nAutovalores:\\n')\n",
    "print('Primer autovalor:', eigenvalues[0])\n",
    "print('Segundo autovalor:', eigenvalues[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=sample[:, 0],\n",
    "    y=sample[:, 1],\n",
    "    size=[1] * sample.shape[0]\n",
    ")\n",
    "\n",
    "# Add another scatter plot with cross markers\n",
    "figure.add_scatter(\n",
    "    x=eigenvectors[:, 0],\n",
    "    y=eigenvectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=25, color='black'),\n",
    "    name='Autovectores'\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los componentes principales para este caso son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos PCA\n",
    "pca = PCA(n_components=2).fit(sample)\n",
    "\n",
    "# Calculamos la proporción de la varianza explicada por cada componente\n",
    "var = pca.explained_variance_ratio_\n",
    "\n",
    "# Transformamos la muestra\n",
    "pca = pca.transform(sample)\n",
    "\n",
    "cols = [f'PC{i+1} ({v:.2f}%)' for i, v in enumerate(var * 100)]\n",
    "pca = pd.DataFrame(pca, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca.iloc[:, 0],\n",
    "    y=pca.iloc[:, 1],\n",
    "    size=[1] * sample.shape[0]\n",
    ")\n",
    "\n",
    "# # Add another scatter plot with cross markers\n",
    "# figure.add_scatter(\n",
    "#     x=eigenvectors[:, 0],\n",
    "#     y=eigenvectors[:, 1],\n",
    "#     mode='markers',\n",
    "#     marker=dict(size=25, color='black'),\n",
    "#     name='Autovectores'\n",
    "# )\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora agregamos un outlier en la misma dirección que el segundo vector obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = np.array([np.mean(sample[:, 0]), 3])\n",
    "sample = np.vstack((sample, outlier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los nuevos PCA y los graficamos para ver como cambiaron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos PCA\n",
    "pca2 = PCA(n_components=2).fit(sample)\n",
    "\n",
    "# Calculamos la proporción de la varianza explicada por cada componente\n",
    "var = pca2.explained_variance_ratio_\n",
    "\n",
    "# Transformamos la muestra\n",
    "pca2 = pca2.transform(sample)\n",
    "\n",
    "cols = [f'PC{i+1} ({v:.2f}%)' for i, v in enumerate(var * 100)]\n",
    "pca2 = pd.DataFrame(pca2, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca.iloc[:, 0],\n",
    "    y=pca.iloc[:, 1],\n",
    "    size=[1] * pca.shape[0],\n",
    ")\n",
    "\n",
    "figure.data[0].name = 'Old Components'\n",
    "figure.data[0].showlegend = True\n",
    "\n",
    "# Add another scatter plot with cross markers\n",
    "figure.add_scatter(\n",
    "    x=pca2.iloc[:, 0],\n",
    "    y=pca2.iloc[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=25, color='red'),\n",
    "    name='New Components',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "*Utilizar las componentes principales como técnica de reducción de dimensión en algún conjunto de datos que utilizaron en Aprendizaje Supervisado. Hacer un ligero análisis y combinar con una técnica de clasificación. Comparar con la misma técnica sin utilizar la reducción. Sugerencia: si utilizó un algoritmo muy complicado, es preferible que baje un poco la complejidad para poder comparar razonablemente.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.load_iris(as_frame=True)['data']\n",
    "y = datasets.load_iris(as_frame=True)['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los componentes principales. Para obtener la cantidad $q$ óptima de componentes miramos la proporción total de la variación explicada por los $q$ componentes y nos quedamos con el valor de $q$ que nos permita explicar aproximadamente un poco más del $90\\%$ de la variación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = []\n",
    "for i in np.arange(2, 5):\n",
    "    pca = PCA(n_components=i).fit(X)\n",
    "    explained_variance.append(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "figure = px.line(\n",
    "    x=np.arange(2, 5),\n",
    "    y=explained_variance,\n",
    "    title='Varianza explicada',\n",
    "    labels={'x': 'q', 'y': 'Varianza'},\n",
    "    template='none'\n",
    ")\n",
    "\n",
    "figure.update_layout(\n",
    "    title='Varianza explicada',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con dos componentes principales ya podemos explicar un $97.7\\%$ de la variabilidad, por lo que elegimos $q=2$. Graficamos los componentes principales con las verdaderas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X)\n",
    "pca = pca.transform(X)\n",
    "pca = pd.DataFrame(pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca.iloc[:, 0],\n",
    "    y=pca.iloc[:, 1],\n",
    "    color=y.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846'},\n",
    "    size=[1] * pca.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos ahora un Random Forest para clasificar las observaciones. Lo entrenamos usando los componentes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pca, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una optimización de parámetros para el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier(random_state=42)\n",
    "optim = RandomizedSearchCV(\n",
    "    estimator=rand_forest,\n",
    "    param_distributions={\n",
    "        'n_estimators': np.arange(50, 150),\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': np.arange(5, 20),\n",
    "        'min_samples_split': np.arange(2, 10),\n",
    "        'min_samples_leaf': np.arange(1, 7)\n",
    "    },\n",
    "    random_state=42,\n",
    "    scoring='precision_macro',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "search = optim.fit(pca, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision:', search.best_score_.__format__('.2%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los grupos graficamente, volvemos ajustar el modelo con todos los datos y con los hiperparámetros óptimos, así podemos comparar visualmente con el gráfico de arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**search.best_params_, random_state=42)\n",
    "y_pred = forest.fit(pca, y).predict(pca)\n",
    "\n",
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca.iloc[:, 0],\n",
    "    y=pca.iloc[:, 1],\n",
    "    color=y_pred.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846'},\n",
    "    size=[1] * pca.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Clasificación con Random Forest',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='PC1',\n",
    "    yaxis_title='PC2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "ax.set_title('Matriz de Confusión', loc='left', fontdict={'fontsize': 13, 'fontweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos la misma optimización de parámetros pero sin utilizar componentes principales como features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier(random_state=42)\n",
    "optim = RandomizedSearchCV(\n",
    "    estimator=rand_forest,\n",
    "    param_distributions={\n",
    "        'n_estimators': np.arange(50, 150),\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': np.arange(5, 20),\n",
    "        'min_samples_split': np.arange(2, 10),\n",
    "        'min_samples_leaf': np.arange(1, 7)\n",
    "    },\n",
    "    random_state=42,\n",
    "    scoring='precision_macro',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "search = optim.fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision:', search.best_score_.__format__('.2%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los grupos graficamente, volvemos ajustar el modelo con todos los datos y con los hiperparámetros óptimos, así podemos comparar visualmente con el gráfico de arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**search.best_params_, random_state=42)\n",
    "y_pred = forest.fit(X, y).predict(X)\n",
    "\n",
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca.iloc[:, 0],\n",
    "    y=pca.iloc[:, 1],\n",
    "    color=y_pred.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846'},\n",
    "    size=[1] * pca.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Clasificación con Random Forest',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='PC1',\n",
    "    yaxis_title='PC2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "ax.set_title('Matriz de Confusión', loc='left', fontdict={'fontsize': 13, 'fontweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "*Para los datasets que estuvimos trabajando utilizar PCA para poder dibujar. Recuerde que reducir la dimensión puede ser una buena forma de visualizar los clusters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos PCA al dataset `usa_arrest`. Graficamente nos queda el siguiente gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/usa_arrests.csv').set_index('Unnamed: 0').rename_axis('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(df)\n",
    "labels = [f'PC{i + 1} ({v * 100:.3}%)' for (i, v) in enumerate(pca.explained_variance_ratio_)]\n",
    "pca = pca.transform(df)\n",
    "\n",
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=pca[:, 0],\n",
    "    y=pca[:, 1],\n",
    "    size=[1] * len(pca[:, 0]),\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title=labels[0],\n",
    "    yaxis_title=labels[1],\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos PCA al dataset `mall_customers`. Graficamente nos queda el siguiente gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mall_costumers.csv').set_index('CustomerID')\n",
    "df['Gender'] = np.where(df['Gender']=='Male', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3).fit(df)\n",
    "labels = [f'PC{i + 1} ({v * 100:.3}%)' for (i, v) in enumerate(pca.explained_variance_ratio_)]\n",
    "pca = pca.transform(df)\n",
    "\n",
    "# Existing scatter plot\n",
    "figure = px.scatter_3d(\n",
    "    x=pca[:, 0],\n",
    "    y=pca[:, 1],\n",
    "    z=pca[:, 2],\n",
    "    size=[1] * len(pca[:, 0]),\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribución',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    scene=dict(\n",
    "        xaxis_title=labels[0],\n",
    "        yaxis_title=labels[1],\n",
    "        zaxis_title=labels[2]\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.update_traces(marker=dict(line=dict(width=0)))\n",
    "figure.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
