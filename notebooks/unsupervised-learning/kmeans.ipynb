{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fXnoCTsIqTpl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import multivariate_t, multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, adjusted_rand_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para formatear los números\n",
    "def format_func(value, tick_number):\n",
    "    return f'{value/1000:.1f}K'\n",
    "\n",
    "def cluster_stability(X, est, n_iter=20, random_state=None):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    labels = []\n",
    "    indices = []\n",
    "    for i in range(n_iter):\n",
    "        # Draw bootstrap samples and store indices\n",
    "        sample_indices = rng.randint(0, X.shape[0], X.shape[0])\n",
    "        indices.append(sample_indices)\n",
    "\n",
    "        # Clone the estimator to make sure that we are fitting fresh models\n",
    "        est = clone(est)\n",
    "\n",
    "        # The hasattr() method returns checks if the estimator has a random_state attribute\n",
    "        if hasattr(est, \"random_state\"):\n",
    "            # Randomize estimator if possible\n",
    "            est.random_state = rng.randint(1e5)\n",
    "        \n",
    "        # Creates a bootstrap sample\n",
    "        X_bootstrap = X[sample_indices]\n",
    "\n",
    "        # Fit the clustering model\n",
    "        est.fit(X_bootstrap)\n",
    "\n",
    "        # Store clustering outcome using original indices\n",
    "        relabel = -np.ones(X.shape[0], dtype=np.int)\n",
    "        relabel[sample_indices] = est.labels_\n",
    "        labels.append(relabel)\n",
    "\n",
    "    scores = []\n",
    "    for l, i in zip(labels, indices):\n",
    "        for k, j in zip(labels, indices):\n",
    "            in_both = np.intersect1d(i, j)\n",
    "            scores.append(adjusted_rand_score(l[in_both], k[in_both]))\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "093NplVC2Cp-"
   },
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Probar el algoritmo utilizando distribuciones t-bivariadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAQINyRcsN3O"
   },
   "source": [
    "## Muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8phJhXw5xKWb"
   },
   "outputs": [],
   "source": [
    "# Observaciones\n",
    "n = 50\n",
    "\n",
    "# Medias\n",
    "mu1 = np.array([2, 2])\n",
    "mu2 = np.array([-1, -1])\n",
    "\n",
    "# Covarianza\n",
    "sigma = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "# Miramos el determinante de la matriz de covarianza. Como tiene que se semi-definida positiva, el determinante tiene que ser mayor que 0\n",
    "det = np.linalg.det(sigma)\n",
    "print('Determinante', det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K-8c3Z9xgrm"
   },
   "outputs": [],
   "source": [
    "# Distribuciones\n",
    "x = multivariate_t.rvs(loc=mu1, shape=sigma, size=n, random_state=42)\n",
    "y = multivariate_t.rvs(loc=mu2, shape=sigma, size=n, random_state=42)\n",
    "\n",
    "X = np.concatenate((x, y), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCBWz_Ekyvsm"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5SHQZR7xttG"
   },
   "outputs": [],
   "source": [
    "# Inicializamos el modelo y lo ajustamos\n",
    "kmeans = KMeans(n_clusters=2, max_iter=10, n_init=2, random_state=42)\n",
    "y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmWlgyMl0CaD",
    "outputId": "f27ffe6b-9728-4a9a-c7e6-ebdc5702f7bc"
   },
   "outputs": [],
   "source": [
    "# Miramos los centros de los clusters y la dispersión total intra-grupo\n",
    "print('Centros:\\n', kmeans.cluster_centers_)\n",
    "print('\\nDispersión total intra-grupo:', kmeans.inertia_.__format__('.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico\n",
    "\n",
    "Miramos los datos utilizando los clusters encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846'},\n",
    "    size=[1] * X.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='K-Means Clustering with Additional Scatter Plot',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "Calculamos la dispersión total intra-grupo para distintos valores de $K$. El valor óptimo de $K$ es aquel a partir del cual la reducción en la dispersión total intra-grupo comienza a desacelerarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos, para distintos valores de K, la dispersión total intra-grupo\n",
    "inertia = []\n",
    "for i in np.arange(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, max_iter=10, n_init=2, random_state=42)\n",
    "    y = kmeans.fit_predict(X)\n",
    "\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "figure = px.line(\n",
    "    x=np.arange(1, 11),\n",
    "    y=inertia,\n",
    "    title='Elbow Method',\n",
    "    labels={'x': 'K', 'y': 'Inertia'},\n",
    "    line_shape='linear'\n",
    ")\n",
    "\n",
    "figure.update_layout(\n",
    "    title='Elbow Method',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='K',\n",
    "    yaxis_title='Dispersión total intra-grupo',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el algoritmo con $k=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, max_iter=10, n_init=2, random_state=42)\n",
    "y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846', '2': '#F6AE2D', '3': '#3C7A89'},\n",
    "    size=[1] * X.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='K-Means Clustering with Additional Scatter Plot',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Silhouette\n",
    "\n",
    "Calculamos el índice de Silhouette para cada observación. Esto nos da una idea de que tan compactos y separados están los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_values = silhouette_samples(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(np.unique(y))\n",
    "y_lower, y_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "colors = ['#E65983', '#2D3846', '#F6AE2D', '#3C7A89']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for i in np.arange(n_clusters):\n",
    "    cluster_silhouette_vals = silhouette_values[y == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    y_upper += len(cluster_silhouette_vals)\n",
    "    ax.barh(np.arange(y_lower, y_upper), cluster_silhouette_vals, edgecolor='white', height=1, color=colors[i])\n",
    "    yticks.append((y_lower + y_upper) / 2)\n",
    "    y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "ax.axvline(x=np.mean(silhouette_values), color=\"black\", linestyle=\"--\")\n",
    "ax.set_yticks(yticks, [f'Cluster {i}' for i in np.arange(n_clusters)])\n",
    "ax.set_xlabel('Silhouette Coefficient')\n",
    "ax.set_title('Silhouette Plot', loc='left', fontdict={'fontsize': 16, 'fontweight': 'bold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czwx5mBB2HYE"
   },
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Modificando las matrices de covarianza encontrar un conjunto de datos que logre un mal desempeño del algoritmo. Intentar con una configuración de tres normales bivariadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtckP0fw2Wan"
   },
   "source": [
    "## Muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b65imr1p2Lux",
    "outputId": "447eb4e0-6b34-458d-c695-5c5594289071"
   },
   "outputs": [],
   "source": [
    "# Observaciones\n",
    "n = 50\n",
    "\n",
    "# Medias\n",
    "mu1 = np.array([2, 2])\n",
    "mu2 = np.array([1, 1])\n",
    "mu3 = np.array([0, 0])\n",
    "means = np.array([mu1, mu2, mu3])\n",
    "\n",
    "# Cambiamos la matriz de covarianza. Aumentamos la varianza de X2 variable y reducimos la de X1\n",
    "sigma = np.array([[0.01, 0.09], [0.09, 1]])\n",
    "\n",
    "# Miramos el determinante de la matriz de covarianza. Como tiene que se semi-definida positiva, el determinante tiene que ser mayor que 0\n",
    "det = np.linalg.det(sigma)\n",
    "print('Determinante', det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmgJUFXa2hVt"
   },
   "outputs": [],
   "source": [
    "# Distribuciones\n",
    "x = multivariate_normal.rvs(mean=mu1, cov=sigma, size=n, random_state=42)\n",
    "y = multivariate_normal.rvs(mean=mu2, cov=sigma, size=n, random_state=42)\n",
    "z = multivariate_normal.rvs(mean=mu3, cov=sigma, size=n, random_state=42)\n",
    "\n",
    "X = np.concatenate((x, y, z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    #color=X[:, 1].astype(str),\n",
    "    #color_discrete_map={'0': '#E65983', '1': '#2D3846'},\n",
    "    size=[1] * X.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribution',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSTTzFqy27Dn"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq6Uz08x2qMf"
   },
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "kmeans = KMeans(n_clusters=3, max_iter=10, n_init=2, random_state=42)\n",
    "y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0H-ws4n3BZG",
    "outputId": "e397290e-c665-4f99-fc12-39a20772ab9d"
   },
   "outputs": [],
   "source": [
    "# Miramos los centros de los clusters y la dispersión total intra-grupo\n",
    "print('Centros:\\n', kmeans.cluster_centers_)\n",
    "print('\\nDispersión total intra-grupo:', kmeans.inertia_.__format__('.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6cCJ3nx7tw1"
   },
   "source": [
    "Lo que podemos observar en el siguiente gráfico es que, como el algoritmo utiliza la distancia euclídea, y como busca **minimizar** la dispersión **intra-grupo**, prefiere agrupar calculando las distancias horizontalmente y no verticalmente, como debería ser en realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846', '2': '#4FDFEF'},\n",
    "    size=[1] * X.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='Distribution',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "# Set x-axis and y-axis limits\n",
    "figure.update_xaxes(range=[-1, 3])\n",
    "figure.update_yaxes(range=[-2, 5])\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFCtqV-m9Ylr"
   },
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "¿Es posible que algunos valores atípicos puedan quebrar el algoritmo? Probar agregando algunos outliers al conjunto de datos que usamos al comienzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-Ol7SOW9h7a"
   },
   "source": [
    "## Muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qlITHQI9box"
   },
   "outputs": [],
   "source": [
    "# Observaciones\n",
    "n = 50\n",
    "\n",
    "# Medias\n",
    "mu1 = np.array([2, 2])\n",
    "mu2 = np.array([-1, -1])\n",
    "\n",
    "# Covarianza\n",
    "sigma = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpJGFe3z9nM4"
   },
   "outputs": [],
   "source": [
    "# Distribuciones\n",
    "x = multivariate_t.rvs(loc=mu1, shape=sigma, size=n, random_state=42)\n",
    "y = multivariate_t.rvs(loc=mu2, shape=sigma, size=n, random_state=42)\n",
    "\n",
    "X = np.concatenate((x, y), axis=0)\n",
    "X = np.concatenate((X, np.array([[1000, 1000]])), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MvLWvvx-s4v"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USp_JCfT-UDD"
   },
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "kmeans = KMeans(n_clusters=2, max_iter=10, n_init=2, random_state=42)\n",
    "y = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhFSyhT4ACAy"
   },
   "source": [
    "La idea es que, como el algoritmo busca minimizar la dispersión intra-grupo, prefiere clasificar todas las observaciones en un cluster y el outlier en otro, lo cual afecta el rendimiento del algoritmo significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing scatter plot\n",
    "figure = px.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    color=y.astype(str),\n",
    "    color_discrete_map={'0': '#E65983', '1': '#2D3846', '2': '#4FDFEF'},\n",
    "    size=[1] * X.shape[0],\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "figure.update_layout(\n",
    "    title='K-Means Clustering',\n",
    "    title_font=dict(size=16, family='Arial', color='black', weight='bold'),\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    "    xaxis=dict(showgrid=True, gridcolor='LightGray', showline=True, linecolor='Black', zeroline=True, zerolinecolor='LightGray'),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
