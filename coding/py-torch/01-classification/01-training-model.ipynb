{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4acfcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 22:21:54,409 - INFO - Current device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.config import Config\n",
    "config = Config()\n",
    "from utils.validation import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317a902",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8d3af",
   "metadata": {},
   "source": [
    "- *Cargamos nuestros datos utilizando la función `torch.load()`. Utilizamos el modelo `Tensor` para validar nuestros tensores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4bcb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load('temp/data/train_data.pth').to(device=config.device)\n",
    "test_data = torch.load('temp/data/test_data.pth').to(device=config.device)\n",
    "\n",
    "train_data = Tensor(tensor=train_data, tensor_dimensions=2)\n",
    "test_data = Tensor(tensor=test_data, tensor_dimensions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8e86",
   "metadata": {},
   "source": [
    "## 2. Build the model\n",
    "\n",
    "- *Podemos crear nuestro modelo definiendo una capa oculta a la vez (como en la primera versión). Cuando definamos el método `forward()` vamos a tener que, explícitamente, definir el grafo computacional de nuestra red neuronal.*\n",
    "- *Otra opción es utilizar el módulo `nn.Sequential`. Este módulo nos permite definir de forma implícita el grafo computacional de nuestra red neuronal.*\n",
    "    - *Funciona como un contenedor en donde definimos los módulos (capas) de nuestra red neuronal en el orden que queremos que se ejecuten al llamar el método `forward()`.*\n",
    "    - *Lo bueno es que no tenemos que definir el grafo computacional dentro del método `forward()`, lo que reduce considerablemente el código.* \n",
    "    - *Se puede encontrar más información en la [documentación](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            # Pass the input tensor through the first layer\n",
    "            x = self.layer_1(x)\n",
    "            # Apply the ReLU activation function\n",
    "            x = torch.relu(x)\n",
    "            # Pass the output of the first layer through the second layer\n",
    "            x = self.layer_2(x)\n",
    "            # Apply the sigmoid activation function for binary classification\n",
    "            x = torch.sigmoid(x)\n",
    "\n",
    "            return x\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            return self.model(X)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "model = NeuralNetworkV0(\n",
    "    input_size=train_data.tensor.shape[1],\n",
    "    hidden_size=5,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3719d92",
   "metadata": {},
   "source": [
    "### 2.1. Architecture\n",
    "\n",
    "⚠️ WIP: Explicación de la estructura de la red neuronal y los cálculos matemáticos que ocurren detrás.\n",
    "\n",
    "<img src=\"attachments/simple-linear-nn.png\" width=\"600\" height=\"600\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf513c",
   "metadata": {},
   "source": [
    "### 2.2. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e597",
   "metadata": {},
   "source": [
    "- *Dado que este es un problema de clasificación binaria, vamos a utilizar la Binary Cross-Entropy como función de pérdida.*\n",
    "- *PyTorch tiene disponibles dos clases para esta función: `torch.nn.BCELoss` y `torch.nn.BCELossWithLogits`. La única diferencia entre ambas es que `torch.nn.BCELossWithLogits` recibe los scores del modelo y con ellos computa las probabilidades (con la función de activación Sigmoidea) y la log-verosimilitud, mientras que `torch.nn.BCELoss` recibe directamente las probabilidades.*\n",
    "- *Lo recomendable es utilizar `torch.nn.BCELossWithLogits` por dos razones:*\n",
    "    1. *Nos ahorramos de definir la función de activación dentro de nuestro modelo.*\n",
    "    2. *Computar las probabilidades (i.e., pasar los scores por la función de activación) y luego aplicar logaritmos suele ser una operación muy inestable numéricamente. Al calcular todo al mismo tiempo, reducimos las inestabilidades.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaedc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ed809",
   "metadata": {},
   "source": [
    "### 2.1. Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38c93700",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
