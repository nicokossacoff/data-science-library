{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4acfcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 12:10:53,783 - INFO - Current device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import Config\n",
    "config = Config()\n",
    "from utils.validation import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317a902",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8d3af",
   "metadata": {},
   "source": [
    "- *Cargamos nuestros datos utilizando la función `torch.load()`. Utilizamos el modelo `Tensor` para validar nuestros tensores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bcb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load('temp/data/train_data.pth').to(device=config.device)\n",
    "test_data = torch.load('temp/data/test_data.pth').to(device=config.device)\n",
    "\n",
    "train_data = Tensor(tensor=train_data, tensor_dimensions=2).tensor\n",
    "test_data = Tensor(tensor=test_data, tensor_dimensions=2).tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8231d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8e86",
   "metadata": {},
   "source": [
    "## 2. Build the model\n",
    "\n",
    "- *Podemos crear nuestro modelo definiendo una capa oculta a la vez (como en la primera versión). Cuando definamos el método `forward()` vamos a tener que, explícitamente, definir el grafo computacional de nuestra red neuronal.*\n",
    "- *Otra opción es utilizar el módulo `nn.Sequential`. Este módulo nos permite definir de forma implícita el grafo computacional de nuestra red neuronal.*\n",
    "    - *Funciona como un contenedor en donde definimos los módulos (capas) de nuestra red neuronal en el orden que queremos que se ejecuten al llamar el método `forward()`.*\n",
    "    - *Lo bueno es que no tenemos que definir el grafo computacional dentro del método `forward()`, lo que reduce considerablemente el código.* \n",
    "    - *Se puede encontrar más información en la [documentación](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            # Pass the input tensor through the first layer\n",
    "            x = self.layer_1(x)\n",
    "            # Apply the ReLU activation function\n",
    "            x = torch.relu(x)\n",
    "            # Pass the output of the first layer through the second layer\n",
    "            x = self.layer_2(x)\n",
    "            # Apply the sigmoid activation function for binary classification\n",
    "            x = torch.sigmoid(x)\n",
    "\n",
    "            return x\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(X)\n",
    "\n",
    "model = NeuralNetworkV0(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_size=5,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3719d92",
   "metadata": {},
   "source": [
    "### 2.1. Architecture\n",
    "\n",
    "⚠️ WIP: Explicación de la estructura de la red neuronal y los cálculos matemáticos que ocurren detrás.\n",
    "\n",
    "<img src=\"attachments/simple-linear-nn.png\" width=\"600\" height=\"600\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf513c",
   "metadata": {},
   "source": [
    "### 2.2. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e597",
   "metadata": {},
   "source": [
    "- *Dado que este es un problema de clasificación binaria, vamos a utilizar la Binary Cross-Entropy como función de pérdida.*\n",
    "- *PyTorch tiene disponibles dos módulos para esta función de pérdida: `torch.nn.BCELoss` y `torch.nn.BCELossWithLogits`. La única diferencia entre ambos módulos es que `torch.nn.BCELossWithLogits` recibe los scores del modelo y con eso computa las probabilidades (pasando los scores por la función activación Sigmoidea) y luego computa la log-verosimilitud, mientras que `torch.nn.BCELoss` solo computa la log-verosimilitud (i.e., recibe las probabilidades).*\n",
    "- *Lo recomendable es utilizar `torch.nn.BCELossWithLogits` por dos razones:*\n",
    "    1. *Nos ahorramos de definir la función de activación dentro de nuestro modelo.*\n",
    "    2. *Computar las probabilidades (i.e., pasar los scores por la función de activación) y luego aplicar logaritmos suele ser una operación muy inestable numéricamente. Al calcular todo al mismo tiempo, reducimos las inestabilidades.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaedc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ed809",
   "metadata": {},
   "source": [
    "### 2.1. Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c93700",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc1dd7",
   "metadata": {},
   "source": [
    "### 2.2. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc828a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs and validation steps\n",
    "epochs = 100\n",
    "validation_steps = 10\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    # Training phase\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass.\n",
    "    # The loss function expects the shape of the model's output to be the same as the ground truth.\n",
    "    # Since this is a binary classification problem, the ground truth tensor is of shape (N,)\n",
    "    # We use .squeeze() to ensure the model's output is also of shape (N,).\n",
    "    outputs = model(X_train)\n",
    "    outputs = outputs.squeeze()\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train.float())\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    if (epoch + 1) % validation_steps == 0:\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Inference phase\n",
    "        with torch.inference_mode():\n",
    "            # Forward pass on validation data\n",
    "            val_outputs = model(X_test)\n",
    "            val_outputs = val_outputs.squeeze()\n",
    "\n",
    "            # Compute the validation loss\n",
    "            val_loss = criterion(val_outputs, y_test.float())\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
