{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.validation import TensorModel, BatchModel\n",
    "from utils.architecures import NeuralNetworkV1, NeuralNetworkV2\n",
    "from utils.training import multiclass_validation_step\n",
    "from utils.plots import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97970b13",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "- *Al igual que en la notebook anterior, utilizamos la clase `Config` para guardar los hiperparámetros de nuestro modelo y el dispositivo en donde vamos a entrenarlo.*\n",
    "- *Con la función `wandb.init()` podemos inicializar un nuevo experimento. Especificamos el proyecto, el equipo, el nombre del experimento (para un mejor seguimiento) y otros parámetros de contexto.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 16,\n",
    "    'hidden_size': [32, 16, 8],\n",
    "    'learning_rate': 0.0001,\n",
    "    'n_features': 2,\n",
    "    'n_classes': 4,\n",
    "    'weight_decay': 0.0005\n",
    "}\n",
    "\n",
    "config = Config(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a499e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights & Biases. This will prompt you to enter your API key if not already logged in.\n",
    "wandb.login()\n",
    "\n",
    "# Initialize Weights & Biases. This will start a new run and log the hyperparameters.\n",
    "run = wandb.init(\n",
    "    project='pytorch-bootcamp',\n",
    "    entity='nikossacoff-development',\n",
    "    name='multiclass-classification-v3',\n",
    "    config={\n",
    "        'model': 'MultiClassNetworkV1',\n",
    "        'optimizer': 'Adam',\n",
    "        'criterion': 'CrossEntropyLoss',\n",
    "        'hyperparameters': hyperparameters\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a43ec",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc158bcb",
   "metadata": {},
   "source": [
    "- *Utilizamos la función `torch.load()` para cargar nuestros datos de entrenamiento y evaluación. Nos aseguramos que esten en el dispositivo correcto.*\n",
    "- *Validamos los tensores con el modelo `TensorModel` de Pydantic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41087e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load('temp/data/multiclass-classification/train_data.pth').to(device=config.device)\n",
    "val_data = torch.load('temp/data/multiclass-classification/validation_data.pth').to(device=config.device)\n",
    "eval_data = torch.load('temp/data/multiclass-classification/evaluation_data.pth').to(device=config.device)\n",
    "\n",
    "# Validate tensors\n",
    "train_data = TensorModel(tensor=train_data, tensor_dimensions=2).tensor\n",
    "val_data = TensorModel(tensor=val_data, tensor_dimensions=2).tensor\n",
    "eval_data = TensorModel(tensor=eval_data, tensor_dimensions=2).tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "X_val, y_val = val_data[:, :-1], val_data[:, -1]\n",
    "X_eval, y_eval = eval_data[:, :-1], eval_data[:, -1]\n",
    "\n",
    "logging.info(f\"Training data: {X_train.shape} | Labels: {y_train.shape}\")\n",
    "logging.info(f\"Validation data: {X_val.shape} | Labels: {y_val.shape}\")\n",
    "logging.info(f\"Evaluation data: {X_eval.shape} | Labels: {y_eval.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb89ea2",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd435b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNetworkV1(nn.Module):\n",
    "    def __init__(self, n_features: int, hidden_size: list, n_classes: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(n_features, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size[2], n_classes)\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "model = MultiClassNetworkV1(\n",
    "    n_features=config.n_features,\n",
    "    hidden_size=config.hidden_size,\n",
    "    n_classes=config.n_classes,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7d40c",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b8833",
   "metadata": {},
   "source": [
    "- *Para los problemas de clasificación con más de dos clases, la función de pérdida más utilizada es la Cross-Entropy.*\n",
    "- *Utilizamos la implementación de PyTorch, `nn.CrossEntropyLoss`. Esta función recibe los scores del modelo sin normalizar (es decir, sin convertir a probabilidades) para reducir las inestabilidades numéricas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7986c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cdd71",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc50278",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94151a9e",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset instante for the training data and validation data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "eval_dataset = TensorDataset(X_eval, y_eval)\n",
    "\n",
    "# Create DataLoaders for the training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb3a0a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of steps. We will validate the model every 5 steps.\n",
    "step = 0\n",
    "\n",
    "# Count the number of epochs without improvement.\n",
    "# If the validation loss is lower than the best validation loss (minus a small delta value), we reset the counter to 0.\n",
    "# If the validation loss is not lower than the best validation loss, we increment the counter by 1. When counter = patience, we stop the training.\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in np.arange(config.epochs):\n",
    "    # Accumulate the training and validation loss for this epoch.\n",
    "    train_loss_accum = 0.0\n",
    "    validation_loss_accum = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for (X_train, y_train) in train_loader:\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(outputs, y_train.long())\n",
    "        train_loss_accum += train_loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Calculate the weight norm\n",
    "        weight_norm = torch.norm(torch.stack([torch.norm(param) for param in model.parameters()]))\n",
    "\n",
    "        # Log the metrics\n",
    "        run.log({\n",
    "            'Metrics/Gradient norm': grad_norm,\n",
    "            'Metrics/Weight norm': weight_norm\n",
    "        })\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    ### Inference phase\n",
    "    validation = multiclass_validation_step(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=config.device\n",
    "    )\n",
    "\n",
    "    run.log({\n",
    "        'Metrics/Accuracy': validation['accuracy'],\n",
    "        'Metrics/Precision': validation['precision'],\n",
    "        'Metrics/Recall': validation['recall'],\n",
    "        'Metrics/F1-Score': validation['f1_score']\n",
    "    })\n",
    "\n",
    "    # Average the training and validation loss\n",
    "    train_loss = train_loss_accum / len(train_loader)\n",
    "\n",
    "    # Log the training loss for the whole epoch\n",
    "    run.log({\n",
    "        'Loss/Training': train_loss,\n",
    "        'Loss/Validation': validation['loss']\n",
    "    })\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        logging.info(f\"Epoch: {epoch + 1}/{config.epochs} | Training loss: {train_loss:.4f} | Validation loss: {validation['loss']:.4f}\")\n",
    "        logging.info(f\"Epoch: {epoch + 1}/{config.epochs} | Accuracy: {validation['accuracy']:.4f} | F1-Score: {validation['f1_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd76c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'temp/models/multi-class-nn.pth')\n",
    "\n",
    "# Generate a wandb artifact\n",
    "model_artifact = wandb.Artifact(\n",
    "    name='multi-class-nn',\n",
    "    type='model',\n",
    "    metadata={\n",
    "        'input_size': config.n_features,\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'epochs': config.epochs,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'dropout': config.dropout,\n",
    "        'weight_decay': config.weight_decay\n",
    "    }\n",
    ")\n",
    "\n",
    "model_artifact.add_file('temp/models/multi-class-nn.pth', name='model.pth')\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f225a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
