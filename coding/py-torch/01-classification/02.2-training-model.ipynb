{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4acfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.validation import TensorModel, BatchModel\n",
    "from utils.architecures import NeuralNetworkV1, NeuralNetworkV2\n",
    "from utils.metrics import *\n",
    "from utils.training import validation_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e90240",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 16,\n",
    "    'hidden_size': [128, 64, 32],\n",
    "    'learning_rate': 0.001,\n",
    "    'n_features': 2,\n",
    "    'dropout': 0.1,\n",
    "    'weight_decay': 0.001\n",
    "}\n",
    "\n",
    "config = Config(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Log in to Weights & Biases. This will prompt you to enter your API key if not already logged in.\n",
    "wandb.login()\n",
    "\n",
    "# Initialize Weights & Biases. This will start a new run and log the hyperparameters.\n",
    "run = wandb.init(\n",
    "    project='pytorch-bootcamp',\n",
    "    entity='nikossacoff-development',\n",
    "    name='circles-classification-4',\n",
    "    config={\n",
    "        'model': 'NeuralNetworkV2',\n",
    "        'optimizer': 'Adam',\n",
    "        'criterion': 'BCEWithLogitsLoss',\n",
    "        'hyperparameters': hyperparameters\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317a902",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8d3af",
   "metadata": {},
   "source": [
    "- *Cargamos nuestros datos utilizando la función `torch.load()`. Utilizamos el modelo `Tensor` para validar nuestros tensores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors\n",
    "train_data = torch.load('temp/data/train_data.pth').to(device=config.device)\n",
    "test_data = torch.load('temp/data/test_data.pth').to(device=config.device)\n",
    "\n",
    "train_data = TensorModel(tensor=train_data, tensor_dimensions=2).tensor\n",
    "test_data = TensorModel(tensor=test_data, tensor_dimensions=2).tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8231d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
    "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
    "\n",
    "logging.info(f\"Training data: {X_train.shape} | Labels: {y_train.shape}\")\n",
    "logging.info(f\"Testing data: {X_test.shape} | Labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8e86",
   "metadata": {},
   "source": [
    "## 2. Build the model\n",
    "\n",
    "- *Podemos crear nuestro modelo definiendo una capa oculta a la vez (como en la primera versión). Cuando definamos el método `forward()` vamos a tener que, explícitamente, definir el grafo computacional de nuestra red neuronal.*\n",
    "- *Otra opción es utilizar el módulo `nn.Sequential`. Este módulo nos permite definir de forma implícita el grafo computacional de nuestra red neuronal.*\n",
    "    - *Funciona como un contenedor en donde definimos los módulos (capas) de nuestra red neuronal en el orden que queremos que se ejecuten al llamar el método `forward()`.*\n",
    "    - *Lo bueno es que no tenemos que definir el grafo computacional dentro del método `forward()`, lo que reduce considerablemente el código.* \n",
    "    - *Se puede encontrar más información en la [documentación](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6725b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            # Pass the input tensor through the first layer\n",
    "            x = self.layer_1(x)\n",
    "            # Apply the ReLU activation function\n",
    "            x = torch.relu(x)\n",
    "            # Pass the output of the first layer through the second layer\n",
    "            x = self.layer_2(x)\n",
    "            # Apply the sigmoid activation function for binary classification\n",
    "            x = torch.sigmoid(x)\n",
    "\n",
    "            return x\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "class NeuralNetworkV0(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = NeuralNetworkV2(\n",
    "    input_size=config.n_features,\n",
    "    hidden_size=config.hidden_size,\n",
    "    dropout=config.dropout,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3719d92",
   "metadata": {},
   "source": [
    "### 2.1. Architecture\n",
    "\n",
    "⚠️ WIP: Explicación de la estructura de la red neuronal y los cálculos matemáticos que ocurren detrás.\n",
    "\n",
    "<img src=\"attachments/simple-linear-nn.png\" width=\"600\" height=\"600\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf513c",
   "metadata": {},
   "source": [
    "### 2.2. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e597",
   "metadata": {},
   "source": [
    "- *Dado que este es un problema de clasificación binaria, vamos a utilizar la Binary Cross-Entropy como función de pérdida.*\n",
    "- *PyTorch tiene disponibles dos módulos para esta función de pérdida: `torch.nn.BCELoss` y `torch.nn.BCELossWithLogits`. La única diferencia entre ambos módulos es que `torch.nn.BCELossWithLogits` recibe los scores del modelo y con eso computa las probabilidades (pasando los scores por la función activación Sigmoidea) y luego computa la log-verosimilitud, mientras que `torch.nn.BCELoss` solo computa la log-verosimilitud (i.e., recibe las probabilidades).*\n",
    "- *Lo recomendable es utilizar `torch.nn.BCELossWithLogits` por dos razones:*\n",
    "    1. *Nos ahorramos de definir la función de activación dentro de nuestro modelo.*\n",
    "    2. *Computar las probabilidades (i.e., pasar los scores por la función de activación) y luego aplicar logaritmos suele ser una operación muy inestable numéricamente. Al calcular todo al mismo tiempo, reducimos las inestabilidades.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaedc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ed809",
   "metadata": {},
   "source": [
    "### 2.1. Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c93700",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425a8c2",
   "metadata": {},
   "source": [
    "### 2.2. DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79146c18",
   "metadata": {},
   "source": [
    "- *`torch.utils.data.DataLoader` es un objeto de PyTorch que nos permite iterar sobre nuestros conjunto de datos de manera eficiente.*\n",
    "- *Nos permiten generar batches (i.e., particionar nuestro conjunto de datos en muestras más pequeñas) y paralelizar el procesamiento de los datos, entre otras cosas.*\n",
    "- *Se construyen a partir de un objeto `torch.utils.data.Dataset`. Esta es una clase abstracta que define la lógica de como acceder y, si es necesario, transformar los datos crudos. Se pueden crear módulos (que heredan las funcionalidades de la clase `torch.utils.data.Dataset`) o se pueden utilizar alguno de los módulos ya disponibles.*\n",
    "- *Más información sobre esto en la [documentación](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e85d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset instante for the training data and validation data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders for the training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc1dd7",
   "metadata": {},
   "source": [
    "### 2.3. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47b456",
   "metadata": {},
   "source": [
    "- *Cada cinco pasos (i.e., después de actualizar cinco veces los pesos de nuestra red) calculamos las métricas de validación. De esta manera podemos entender si el modelo está aprendiendo a generalizar a los datos o simplemente está memorizando.*\n",
    "- *Como es un problema de clasificación, además de calcular la función de pérdida para la muestra de entrenamiento y de validación, vamos a calcular las siguientes métricas:*\n",
    "    - **Accuracy**. *Número de observaciones que nuestro modelo clasificó correctamente sobre el total de observaciones.*\n",
    "    - **Precision**. *Número de observaciones que clasificamos correctamente como positiva (i.e., $y=1$) sobre el total de observaciones que clasificamos como positivas. Es una medida de que tan eficiente es nuestro modelo. Al maximizar la precision reducimos los falsos positivos.*\n",
    "    - **Recall**. *Número de observaciones que clasificamos correctamente como positiva (i.e., $y=1$) sobre el total las observaciones que eran positivas. Es una medida de que tanto logra capturar nuestro modelo. Al maximizar la recall reducimos los falsos negativos.*\n",
    "    - **F1-Score**. *Es la media armónica entre la eficiencia (precision) y la captura (recall). Nos dice que tan bien balancea nuestro modelo los falsos positivos y los falsos negativos.*\n",
    "- *También vamos a calcular otras métricas que nos suelen ayudar a entender si el modelo está generalizando bien a los datos o no:*\n",
    "    - **Weight-norm**. *Normalizamos los pesos.*\n",
    "        - *Un incremento en los pesos del modelo puede derivar en un modelo que no generaliza correctamente a los datos. El hacer un seguimiento de los pesos durante el entrenamiento nos permite investigar y entender si nuestro modelo está sobreajustando o no a los datos.*\n",
    "        - *Weight-decay es un método de regularización que nos permite reducir el tamaño de los pesos para evitar el sobreajuste. Podemos evaluar su impacto al hacer un seguimiento de los pesos durante el entrenamiento.*\n",
    "    - **Gradient-norm**. *Normalizamos los gradientes.*\n",
    "        - *Los gradientes de la función de pérdida con respecto a los pesos del modelo nos indican la dirección de mayor crecimiento (nosotros nos queremos mover en la dirección contraria). Por lo general, no nos interesa la magnitud de los gradientes, solo nos interesa la dirección en la que nos tenemos que mover. Se suelen normalizar los gradientes para obtener una mayor estabilidad durante el entrenamiento del modelo, aunque no siempre es la mejor opción (más información en este [posteo](https://stats.stackexchange.com/questions/22568/difference-in-using-normalized-gradient-and-gradient)).*\n",
    "        - *Cosas a tener en cuenta:*\n",
    "            - *Si observamos que la norma de los gradientes es muy cercana a cero (i.e., los gradientes son cercanos a cero), entonces tenemos un problema de vanishing gradients. Si eso ocurre, entonces no estamos actualizando los pesos y, por ende, el modelo deja de aprender.*\n",
    "            - *Si observamos que la norma de los gradientes es muy grande (i.e., los gradientes están creciendo mucho), entonces tenemos un problema de exploding gradients. Esto nos puede alertar de posibles inestabilidades durante el entrenamiento.*\n",
    "            - *Una caída abrupta puede reflejar una convergencia prematura o que el modelo haya llegado a un mínimo local o saddle-point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of steps. We will validate the model every 5 steps.\n",
    "step = 0\n",
    "\n",
    "# Accumulate the training loss \n",
    "train_loss_accum = 0.0\n",
    "\n",
    "# Accumulate the validation loss.\n",
    "validation_loss_accum = 0.0\n",
    "\n",
    "# Count the number of epochs without improvement.\n",
    "# If the validation loss is lower than the best validation loss (minus a small delta value), we reset the counter to 0.\n",
    "# If the validation loss is not lower than the best validation loss, we increment the counter by 1. When counter = patience, we stop the training.\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in np.arange(config.epochs):\n",
    "    # Accumulate the training and validation loss \n",
    "    train_loss_accum = 0.0\n",
    "    validation_loss_accum = 0.0\n",
    "\n",
    "    for (X_train, y_train) in train_loader:\n",
    "        ### Training phase\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass.\n",
    "        # The loss function expects the shape of the model's output to be the same as the ground truth.\n",
    "        # Since this is a binary classification problem, the ground truth tensor is of shape (N,)\n",
    "        # We use .squeeze() to ensure the model's output is also of shape (N,).\n",
    "        outputs = model(X_train)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(outputs, y_train.float())\n",
    "        # Accumulate the training loss\n",
    "        train_loss_accum += train_loss.item()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Calculate the norm of the gradients. We don't clip the gradients here, but you can do so if needed.\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Calculate the norm of the weights\n",
    "        weight_norm = torch.norm(torch.stack([torch.norm(param) for param in model.parameters()]))\n",
    "\n",
    "        # Log the gradients and weights norms\n",
    "        run.log({\n",
    "            'Metrics/Gradient norm': grad_norm,\n",
    "            'Metrics/Weight norm': weight_norm\n",
    "        })\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    ### Inference phase\n",
    "    val_results = validation_step(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=config.device\n",
    "    )\n",
    "\n",
    "    run.log({\n",
    "        'Metrics/Accuracy': val_results['accuracy'],\n",
    "        'Metrics/Precision': val_results['precision'],\n",
    "        'Metrics/Recall': val_results['recall'],\n",
    "        'Metrics/F1-Score': val_results['f1_score']\n",
    "    })\n",
    "\n",
    "    # Average the training and validation loss\n",
    "    train_loss = train_loss_accum / len(train_loader)\n",
    "\n",
    "    # Log the training loss for the whole epoch\n",
    "    run.log({\n",
    "        'Loss/Training': train_loss,\n",
    "        'Loss/Validation': val_results['loss']\n",
    "    })\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        logging.info(f\"Epoch: {epoch + 1}/{config.epochs} | Training loss: {train_loss:.4f} | Validation loss: {val_results['loss']:.4f}\")\n",
    "        logging.info(f\"Epoch: {epoch + 1}/{config.epochs} | Accuracy: {val_results['accuracy']:.4f} | F1-Score: {val_results['f1_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13118000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'temp/models/neural-network-v2.pth')\n",
    "\n",
    "# Generate a wandb artifact\n",
    "model_artifact = wandb.Artifact(\n",
    "    name='neural-network-v2',\n",
    "    type='model',\n",
    "    metadata={\n",
    "        'input_size': config.n_features,\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'epochs': config.epochs,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'dropout': config.dropout,\n",
    "        'weight_decay': config.weight_decay\n",
    "    }\n",
    ")\n",
    "\n",
    "model_artifact.add_file('temp/models/neural-network-v2.pth', name='model.pth')\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()  # Finish the Weights & Biases run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
