{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae4ab6cf7d5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac9aa4b767f9c7",
   "metadata": {},
   "source": [
    "# Modifying tensors\n",
    "\n",
    "1. *View (`torch.Tensor.view`). Allows us to change the shape of the tensor, similar to what the `torch.Tensor.reshape()` method does, but with two major differences:*\n",
    "    - *It doesn't make a copy of the tensor in memory. The new tensor references the same object in memory.*\n",
    "    - *It only works if the tensor is **contiguous** in memory.*\n",
    "        - *A tensor being contiguous means that its elements are stored sequentially in an uninterrupted block of memory (one-dimensional vector).*\n",
    "    - *There are operations that can make a tensor non-contiguous, for example, transposing a tensor. In this case, this method will return an error (you can call the `torch.Tensor.contiguous()` method before calling `torch.Tensor.view()`).*\n",
    "2. *Reshape (`torch.Tensor.reshape()`). Allows us to change the shape of the tensor without changing its elements. For example, changing the dimension from $3\\times 2$ to $2\\times 3$.*\n",
    "    - *Doesn't require the tensor to be contiguous in memory. If the tensor is contiguous, it calls the `torch.Tensor.view()` method. If it's not contiguous, then it first calls the `torch.Tensor.contiguous()` method, creating a copy of the tensor in memory that is contiguous.*\n",
    "    - *If we're not sure whether the tensor is contiguous or not, it's better to use `torch.Tensor.reshape()` instead of `torch.Tensor.view()`.*\n",
    "3. *Concatenate. There are several options available for concatenating tensors:*\n",
    "    -  *`torch.cat()`. Allows concatenating tensors along an existing dimension, resulting in a tensor with the same number of dimensions. For example, if we have two $2\\times 2$ tensors, and we concatenate them using `dim=0` (in this case rows), the result will be a $4\\times 2$ tensor, while if we concatenate them using `dim=1` (in this case columns), the result will be a $2\\times 4$ tensor.*\n",
    "        - *It's not necessary for the dimension along which we concatenate to have the same size, but the remaining dimensions must be equal.*\n",
    "    - *`torch.stack()`. Allows concatenating tensors along a new dimension, resulting in a tensor with an additional dimension. For example, if we have two $2\\times 2$ tensors, and we concatenate them using `dim=0`, then it returns a tensor of dimension $2\\times2\\times2$.*\n",
    "        - *Both tensors must have the same size.*\n",
    "4. *Remove/add dimensions. The `torch.squeeze()` function allows us to remove dimensions of size 1, while the `torch.unsqueeze()` function allows us to add dimensions of size 1.*\n",
    "5. *Permute dimensions. With the `torch.permute()` function we can rearrange the dimensions of a tensor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c8086675c2bab",
   "metadata": {},
   "source": [
    "## Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f0d33a832b3d459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:48:56.298999Z",
     "start_time": "2025-04-20T15:48:56.294123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(0, 10)\n",
    "X = X.type(torch.float32)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6fc414ac59ebc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:48:57.488619Z",
     "start_time": "2025-04-20T15:48:57.484642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X.view(2, 5)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0b3489df502e5",
   "metadata": {},
   "source": [
    "- *A new tensor `y` was created as a view of tensor `X`, with shape $2\\times 5$. Modifying any element of tensor `y` will also affect tensor `X`, since `y` shares the same underlying memory as `X`. This behavior occurs because views reference the original tensor's data rather than creating a copy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d06f9b5d11278fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:49:00.652657Z",
     "start_time": "2025-04-20T15:49:00.647013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  1.,  2.,  3.,  4.],\n",
       "        [20.,  6.,  7.,  8.,  9.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 0] = torch.tensor([10, 20], dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3df138aae01fd74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:49:01.753402Z",
     "start_time": "2025-04-20T15:49:01.748975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  1.,  2.,  3.,  4., 20.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4145fb80855db",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d802423036176af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:54:36.826899Z",
     "start_time": "2025-04-20T15:54:36.823841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3700, 0.5732, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(2, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cacdb245f20cfee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:54:46.110464Z",
     "start_time": "2025-04-20T15:54:46.106634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3700, 0.5732],\n",
       "        [0.2554, 0.2901],\n",
       "        [0.9397, 0.7976]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X.reshape(3, -1) # -1 means that the size of that dimension is defined automatically\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed035d53c63de10d",
   "metadata": {},
   "source": [
    "- *Here, the `reshape` method gives us a view of tensor `X`, so any changes we make to `y` will also show up in `X`. That's because `X` is still stored contiguously in memory, letting `reshape` just reference the original data instead of making a copy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c9ffb13cfe009d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:55:39.113841Z",
     "start_time": "2025-04-20T15:55:39.109269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000],\n",
       "        [0.2554, 0.2901],\n",
       "        [0.9397, 0.7976]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0, :] = torch.tensor([1, 1])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ecd7a01a37006426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:55:43.052766Z",
     "start_time": "2025-04-20T15:55:43.048381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd97b4313189a0",
   "metadata": {},
   "source": [
    "- *We create a new tensor `z` by transposing `X`, and then generate tensor `y` from `z`. If we modify `y`, those changes won’t affect `X`. Why? Because transposing `X` makes `z` non-contiguous in memory, so when we call `reshape()` on `z`, PyTorch creates a copy instead of a view. That’s why `y` is independent from `X`—they no longer share the same underlying data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0e1414dcf99e3a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:22.326646Z",
     "start_time": "2025-04-20T15:56:22.322281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2901],\n",
       "        [1.0000, 0.9397],\n",
       "        [0.2554, 0.7976]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = X.T\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c299133b9ec7d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:31.390496Z",
     "start_time": "2025-04-20T15:56:31.385737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2901, 1.0000],\n",
       "        [0.9397, 0.2554, 0.7976]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = z.reshape(2, -1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4639516002c4b2ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:55.731344Z",
     "start_time": "2025-04-20T15:56:55.726587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2901, 1.0000],\n",
       "        [0.0000, 0.2554, 0.7976]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 0] = torch.tensor([0, 0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca8587f737bd81c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:59:26.492420Z",
     "start_time": "2025-04-20T15:59:26.488714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1212f80c4dc96f4",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d156f89c95392ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:45:54.029629Z",
     "start_time": "2025-04-20T17:45:54.020957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_A:\n",
      " tensor([[0.9063, 0.8582, 0.9965],\n",
      "        [0.8017, 0.4278, 0.9973]])\n",
      "tensor_B:\n",
      " tensor([[0.9701, 0.1752, 0.0940],\n",
      "        [0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "source": [
    "tensor_A = torch.rand(size=(2, 3))\n",
    "tensor_B = torch.rand(size=(2, 3))\n",
    "\n",
    "print(f'tensor_A:\\n {tensor_A}')\n",
    "print(f'tensor_B:\\n {tensor_B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2dff0dc185717",
   "metadata": {},
   "source": [
    "- *If we use `torch.cat()`, we can concatenate two tensors along an existing dimension (maintains the size of the dimensions along which we don't concatenate). For example, if we concatenate along `dim=0`, we get a $4\\times 2$ tensor (the size of the second dimension is maintained).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3db4c5ae034788bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:47:17.123423Z",
     "start_time": "2025-04-20T17:47:17.119979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([4, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9063, 0.8582, 0.9965],\n",
      "        [0.8017, 0.4278, 0.9973],\n",
      "        [0.9701, 0.1752, 0.0940],\n",
      "        [0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "source": [
    "tensor_C = torch.cat((tensor_A, tensor_B), dim=0)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618afb8c9a8340a",
   "metadata": {},
   "source": [
    "*If, on the other hand, we use dimension `dim=1`, we get a $2\\times6$ tensor.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe950f633e4165af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:49:10.010820Z",
     "start_time": "2025-04-20T17:49:10.006694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([2, 6])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9063, 0.8582, 0.9965, 0.9701, 0.1752, 0.0940],\n",
      "        [0.8017, 0.4278, 0.9973, 0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "source": [
    "tensor_C = torch.cat((tensor_A, tensor_B), dim=1)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05e1dfd094996a",
   "metadata": {},
   "source": [
    "- *If we use `torch.stack()`, we create a new dimension. So, in this case we get a three-dimensional tensor from concatenating two two-dimensional tensors. In this case it's necessary that both tensors have the same dimensions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "47ecfd256f4b4a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:53:17.158865Z",
     "start_time": "2025-04-20T17:53:17.153760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([2, 2, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9063, 0.8582, 0.9965],\n",
      "         [0.8017, 0.4278, 0.9973]],\n",
      "\n",
      "        [[0.9701, 0.1752, 0.0940],\n",
      "         [0.1659, 0.8378, 0.3756]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_C = torch.stack((tensor_A, tensor_B), dim=0)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e172b9cb909a2ea",
   "metadata": {},
   "source": [
    "## Remove/add dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc295a5426e0f142",
   "metadata": {},
   "source": [
    "- *The `torch.squeeze()` function lets you drop dimensions of size 1 from a tensor. You can use the `dim` argument to specify exactly which dimensions to remove.*\n",
    "- *Remember, the result is a view (`torch.Tensor.view`), so it shares memory with the original tensor—any changes you make will affect both.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3215589b47dc74d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:19:24.617166Z",
     "start_time": "2025-04-20T18:19:24.613618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1866, 0.0648, 0.6463],\n",
       "         [0.6808, 0.5452, 0.9616]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 2, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9caa8cf1694332",
   "metadata": {},
   "source": [
    "- *We dropped the first dimension (size 1), so now `tensor_X_squeezed` is a $2\\times 3$ matrix. This makes the tensor easier to work with, since those singleton dimensions often just get in the way.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba256914e05d08bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:13:44.486391Z",
     "start_time": "2025-04-20T18:13:44.483407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 2, 3])\n",
      "Shape after squeezing: torch.Size([2, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9786, 0.7882, 0.6897],\n",
      "        [0.1621, 0.8248, 0.8890]])\n"
     ]
    }
   ],
   "source": [
    "tensor_X_squeezed = torch.squeeze(X)\n",
    "print(f'Original shape: {X.shape}')\n",
    "print(f'Shape after squeezing: {tensor_X_squeezed.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_X_squeezed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce183fcbb3349c4",
   "metadata": {},
   "source": [
    "- *On the other hand, `torch.unsqueeze()` lets you add a dimension wherever you need it—just specify which one. In the example below, we’re putting back the dimension we squeezed out earlier. This is super handy for prepping tensors for operations that expect a certain shape.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "98cae64360ee19ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:17:43.788727Z",
     "start_time": "2025-04-20T18:17:43.786137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape squeezed tensor: torch.Size([2, 3])\n",
      "Shape after unsqueezing: torch.Size([2, 1, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9786, 0.7882, 0.6897]],\n",
      "\n",
      "        [[0.1621, 0.8248, 0.8890]]])\n"
     ]
    }
   ],
   "source": [
    "tensor_X_unsqueezed = torch.unsqueeze(tensor_X_squeezed, dim=0)\n",
    "print(f'Shape squeezed tensor: {tensor_X_squeezed.shape}')\n",
    "print(f'Shape after unsqueezing: {tensor_X_unsqueezed.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_X_unsqueezed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f72629bd7e74e8",
   "metadata": {},
   "source": [
    "## Permute dimensions\n",
    "\n",
    "- *With `torch.permute()`, you can shuffle the order of a tensor’s dimensions however you want. For example, suppose you have a $1024 \\times 1024 \\times 3$ tensor—think of it as an image, where the first two dimensions are height and width, and the last one is the color channels (RGB). If you need to move the channel dimension to the front (so it’s $3 \\times 1024 \\times 1024$), just specify the new order of axes. This is super useful for prepping data for deep learning models that expect a specific format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e7b0a8cb115f1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:31:54.669623Z",
     "start_time": "2025-04-20T18:31:54.654483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9683, 0.9730, 0.2627],\n",
       "         [0.8097, 0.8765, 0.0550],\n",
       "         [0.6498, 0.4069, 0.7136],\n",
       "         ...,\n",
       "         [0.3865, 0.8466, 0.9223],\n",
       "         [0.7619, 0.6037, 0.4737],\n",
       "         [0.6288, 0.7732, 0.5016]],\n",
       "\n",
       "        [[0.7348, 0.9181, 0.7637],\n",
       "         [0.3168, 0.6106, 0.4738],\n",
       "         [0.4297, 0.4724, 0.0670],\n",
       "         ...,\n",
       "         [0.6849, 0.3207, 0.5561],\n",
       "         [0.5420, 0.3546, 0.4909],\n",
       "         [0.5327, 0.2005, 0.7087]],\n",
       "\n",
       "        [[0.5963, 0.4694, 0.3344],\n",
       "         [0.8822, 0.2125, 0.4717],\n",
       "         [0.0236, 0.4680, 0.9261],\n",
       "         ...,\n",
       "         [0.5447, 0.1777, 0.9908],\n",
       "         [0.6046, 0.7683, 0.6753],\n",
       "         [0.4742, 0.0015, 0.0772]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8567, 0.7679, 0.7789],\n",
       "         [0.4698, 0.0566, 0.6014],\n",
       "         [0.8258, 0.4664, 0.5827],\n",
       "         ...,\n",
       "         [0.7187, 0.1043, 0.7711],\n",
       "         [0.3493, 0.7985, 0.1038],\n",
       "         [0.0346, 0.3198, 0.4269]],\n",
       "\n",
       "        [[0.6278, 0.5091, 0.4229],\n",
       "         [0.0168, 0.8149, 0.7544],\n",
       "         [0.8417, 0.5538, 0.8600],\n",
       "         ...,\n",
       "         [0.7759, 0.1778, 0.3747],\n",
       "         [0.0819, 0.9297, 0.4055],\n",
       "         [0.1806, 0.2597, 0.5550]],\n",
       "\n",
       "        [[0.7465, 0.3158, 0.8514],\n",
       "         [0.6822, 0.2986, 0.1693],\n",
       "         [0.9011, 0.9336, 0.3586],\n",
       "         ...,\n",
       "         [0.5210, 0.7761, 0.0670],\n",
       "         [0.7169, 0.6780, 0.8586],\n",
       "         [0.2203, 0.3433, 0.3857]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(size=(1024, 1024, 3))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4b8d5bb98644e",
   "metadata": {},
   "source": [
    "- *To permute tensor dimensions, just list the axes in the order you want them to appear in the new tensor. Here, we’re moving the third dimension (channels, `dim=2`) to the front, followed by the first (`dim=0`) and second (`dim=1`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3da04e1614a5898b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:36:53.786406Z",
     "start_time": "2025-04-20T18:36:53.781308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1024, 1024, 3])\n",
      "Shape after permuting: torch.Size([3, 1024, 1024])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9683, 0.8097, 0.6498,  ..., 0.3865, 0.7619, 0.6288],\n",
      "         [0.7348, 0.3168, 0.4297,  ..., 0.6849, 0.5420, 0.5327],\n",
      "         [0.5963, 0.8822, 0.0236,  ..., 0.5447, 0.6046, 0.4742],\n",
      "         ...,\n",
      "         [0.8567, 0.4698, 0.8258,  ..., 0.7187, 0.3493, 0.0346],\n",
      "         [0.6278, 0.0168, 0.8417,  ..., 0.7759, 0.0819, 0.1806],\n",
      "         [0.7465, 0.6822, 0.9011,  ..., 0.5210, 0.7169, 0.2203]],\n",
      "\n",
      "        [[0.9730, 0.8765, 0.4069,  ..., 0.8466, 0.6037, 0.7732],\n",
      "         [0.9181, 0.6106, 0.4724,  ..., 0.3207, 0.3546, 0.2005],\n",
      "         [0.4694, 0.2125, 0.4680,  ..., 0.1777, 0.7683, 0.0015],\n",
      "         ...,\n",
      "         [0.7679, 0.0566, 0.4664,  ..., 0.1043, 0.7985, 0.3198],\n",
      "         [0.5091, 0.8149, 0.5538,  ..., 0.1778, 0.9297, 0.2597],\n",
      "         [0.3158, 0.2986, 0.9336,  ..., 0.7761, 0.6780, 0.3433]],\n",
      "\n",
      "        [[0.2627, 0.0550, 0.7136,  ..., 0.9223, 0.4737, 0.5016],\n",
      "         [0.7637, 0.4738, 0.0670,  ..., 0.5561, 0.4909, 0.7087],\n",
      "         [0.3344, 0.4717, 0.9261,  ..., 0.9908, 0.6753, 0.0772],\n",
      "         ...,\n",
      "         [0.7789, 0.6014, 0.5827,  ..., 0.7711, 0.1038, 0.4269],\n",
      "         [0.4229, 0.7544, 0.8600,  ..., 0.3747, 0.4055, 0.5550],\n",
      "         [0.8514, 0.1693, 0.3586,  ..., 0.0670, 0.8586, 0.3857]]])\n"
     ]
    }
   ],
   "source": [
    "image_permuted = torch.permute(image, dims=(2, 0, 1))\n",
    "print(f'Original shape: {image.shape}')\n",
    "print(f'Shape after permuting: {image_permuted.shape}\\n')\n",
    "print(f'New tensor:\\n {image_permuted}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
