{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d134a765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:11:04.275625Z",
     "start_time": "2025-05-04T22:11:03.240846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "Is MPS available? True\n",
      "Does MPS exists? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Is MPS available? {torch.backends.mps.is_available()}')\n",
    "print(f'Does MPS exists? {torch.backends.mps.is_built()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc324fe8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- *Neural networks are powerful tools for tackling a wide variety of problems, thanks to their ability to learn and extract patterns from data. For instance, they can be used to detect tumors in medical images or convert text into audio.*\n",
    "- *How do we train these models? By example. Imagine you want to teach a neural network to distinguish between dogs and cats. The first step is to collect a large set of images containing both animals, and label each image as either \"dog\" or \"cat.\" The model uses these image-label pairs to learn the patterns and features that differentiate the two classes. After training, we compare the model’s predictions to the actual labels and adjust its parameters to improve accuracy.*\n",
    "- *Here’s the catch: computers don’t understand images, text, or audio the way humans do—they only process numbers. So, before training a neural network, we need to convert our data into a numerical format that the computer can work with.*\n",
    "- *PyTorch makes this easy by providing a data structure called a **tensor**. Tensors let us represent all kinds of data—images, text, audio—as numbers. In PyTorch, a tensor is essentially a **multidimensional array** (think NumPy arrays), but with extra features that make them ideal for deep learning tasks.*\n",
    "\n",
    "<img src=\"attachments/tensors-in-pytorch.png\" width=\"1400\" height=\"600\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40869d4b",
   "metadata": {},
   "source": [
    "## Create tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09544bd1",
   "metadata": {},
   "source": [
    "### Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7328fc33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:11:09.549494Z",
     "start_time": "2025-05-04T22:11:09.525326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a scalar\n",
    "scalar = torch.tensor(7.0)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d49cea9c92de7",
   "metadata": {},
   "source": [
    "- *Tensors are n-dimensional arrays. The number of dimensions tells us how many indices we need to access a single value. For example, a scalar is a 0-dimensional tensor—no indices required to get its value.*\n",
    "- *As we add dimensions, we move from scalars to vectors, matrices, and beyond.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736e540971d83f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:11:35.092411Z",
     "start_time": "2025-05-04T22:11:35.089331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75fb03",
   "metadata": {},
   "source": [
    "- *You can extract the number inside the tensor with the `item()` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20f2a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:16:17.261647Z",
     "start_time": "2025-05-04T22:16:17.258431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escalar: 7.0. Type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# Get a tensor back as an integer or float\n",
    "print(f'Escalar: {scalar.item()}. Type: {type(scalar.item())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cf8f8",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "- *Vectors are **one-dimensional** tensors. Using the `.ndim` attribute, we can see that the tensor we created has a single dimension (because to get a scalar we need a single index).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237657da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:17:52.641320Z",
     "start_time": "2025-05-04T22:17:52.636868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector = tensor([7, 7])\n",
      "Dimensions: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a vector\n",
    "vector = torch.tensor([7, 7])\n",
    "print(f'Vector = {vector}')\n",
    "print(f'Dimensions: {vector.ndim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be69fca873dfefe",
   "metadata": {},
   "source": [
    "- *If we use the `.shape` attribute, we can see the size of each dimension. In this case, our tensor has only one dimension with two elements.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3646aa57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:19:49.891786Z",
     "start_time": "2025-05-04T22:19:49.888944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape: {vector.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742257f",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "- *Matrices are **two-dimensional** tensors, where the first dimension represents rows and the second dimension represents columns. The size of each dimension refers to the number of rows and columns in the matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615852d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:21:22.744944Z",
     "start_time": "2025-05-04T22:21:22.741520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix =\n",
      " tensor([[7, 8],\n",
      "        [8, 9]])\n",
      "Dimensions: 2\n",
      "Shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "MATRIX = torch.tensor([[7, 8], [8, 9]])\n",
    "print(f'Matrix =\\n {MATRIX}')\n",
    "print(f'Dimensions: {MATRIX.ndim}')\n",
    "print(f'Shape: {MATRIX.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19f8da",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "- *While vectors and matrices are, at least in PyTorch, 1D and 2D tensors, respectively, we call **n-dimensional** arrays (with $n > 2$) tensors. For example, below we have a three-dimensional tensor.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e66ad65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:22:07.061843Z",
     "start_time": "2025-05-04T22:22:07.057079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8506706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:22:19.354754Z",
     "start_time": "2025-05-04T22:22:19.351990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 3\n",
      "Shape: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensions: {TENSOR.ndim}')\n",
    "print(f'Shape: {TENSOR.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae651728",
   "metadata": {},
   "source": [
    "- *To create a tensor using other tensor(s) as input(s), you need to use functions like `torch.stack()` or `torch.vstack()`.*\n",
    "- *The `torch.tensor()` class only accepts native Python objects. In this case, we use the `torch.stack()` function which allows us to concatenate tensors along a new dimension.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8480b4ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:24:09.477952Z",
     "start_time": "2025-05-04T22:24:09.472232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of A = 2, Shape of A = torch.Size([3, 3])\n",
      "Dimension of B = 2, Shape of B = torch.Size([3, 3])\n",
      "Dimension of TENSOR = 3, Shape of TENSOR = torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = torch.tensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n",
    "\n",
    "TENSOR = torch.stack([A, B])\n",
    "\n",
    "print(f'Dimension of A = {A.ndim}, Shape of A = {A.shape}')\n",
    "print(f'Dimension of B = {B.ndim}, Shape of B = {B.shape}')\n",
    "print(f'Dimension of TENSOR = {TENSOR.ndim}, Shape of TENSOR = {TENSOR.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "badf4df8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:24:24.013459Z",
     "start_time": "2025-05-04T22:24:24.009875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6],\n",
       "         [ 7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12],\n",
       "         [13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fae490aa057df1",
   "metadata": {},
   "source": [
    "## Tensors in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295677d0c032129a",
   "metadata": {},
   "source": [
    "- *Unlike native Python data structures like lists, tensors are stored in memory as a **single contiguous block**. What does this mean? Unlike lists, where each element is a Python object that has its own space in memory, tensors are stored in a single memory space. That is, no matter how many dimensions our tensor has, it's stored in memory as a one-dimensional vector where each element is located next to each other. This makes it much faster to sequentially access elements in a tensor and much easier to vectorize operations with them.*\n",
    "\n",
    "- *This is related to PyTorch's architecture. When we create an instance of `torch.Tensor`, we also create an instance of `torch.Storage`. This `torch.Storage` instance is nothing more than a one-dimensional vector that contains the data in memory. The `torch.Tensor` instance, which is what we users interact with, is a view of the `torch.Storage` instance, that is, a view of the data in memory. This makes PyTorch very efficient in data handling, because it allows us to create multiple tensors without having to duplicate our data in memory (i.e., we only need to create a new reference to the object in memory).*\n",
    "\n",
    "- *The following image is very representative of how a list (or other native Python objects) is stored in memory compared to tensors:*\n",
    "<figure>\n",
    "    <img src=\"attachments/tensors-in-memory.png\" width=\"1400\" height=\"600\" align=\"center\" style=\"display: block; margin: 0 auto;\"/>\n",
    "    <figcaption style=\"text-align: center;\">Stevens, Eli. (2020). Python object (boxed) numeric values versus tensor (unboxed array) numeric values. In Stevens, Eli, <i>Deep Learning with PyTorch</i> (p. 44).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af764ede7c8b8698",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3196e3431d149c",
   "metadata": {},
   "source": [
    "- *As we already mentioned, tensors are stored in memory as one-dimensional vectors, even though we may have created them with more than one dimension. To be able to view tensors with the desired dimensions, the `torch.Tensor` object contains several important attributes:*\n",
    "    - *`size`. Tells us how many elements the tensor has in each dimension.*\n",
    "    - *`stride`. Tells us how many elements in memory we need to move to get the next element of a dimension. For example, in the image below we can see that to get the next element in the second dimension (columns) we need to move one place in memory, and to get the next element in the first dimension (rows) we need to move three places in memory.*\n",
    "    - *`offset`. Tells us what the first element of the tensor is in memory.*\n",
    "    - *`storage`. Tells us where the tensor is stored in memory.*\n",
    "<figure>\n",
    "    <img src=\"attachments/tensor-metadata.png\" width=\"900\" height=\"700\" align=\"center\" style=\"display: block; margin: 0 auto;\"/>\n",
    "    <figcaption style=\"text-align: center;\">Stevens, Eli. (2020). Relationship between tensor offser, size, and stride. In Stevens, Eli, <i>Deep Learning with PyTorch</i> (p. 56).</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db5963",
   "metadata": {},
   "source": [
    "## Random tensors\n",
    "\n",
    "- *Why do we want to learn to generate random tensors? Because neural network weights are initialized this way. So, when starting the training of our neural network, we must pass it a tensor that represents the initial weights, which we need to be able to generate randomly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4f394da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:53:30.394736Z",
     "start_time": "2025-05-04T22:53:30.391845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_tensor =\n",
      " tensor([[0.6386, 0.5047, 0.0468],\n",
      "        [0.9551, 0.8437, 0.5572]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor de dos dimensiones de tamaño (3, 4)\n",
    "random_tensor = torch.rand(size=(2, 3))\n",
    "print(f'random_tensor =\\n {random_tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbff825a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:53:38.854083Z",
     "start_time": "2025-05-04T22:53:38.852300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 2\n",
      "Size: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensions: {random_tensor.ndim}')\n",
    "print(f'Size: {random_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e49668e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:53:47.597911Z",
     "start_time": "2025-05-04T22:53:47.594731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_tensor =\n",
      " tensor([[[0.2895, 0.8592, 0.3154],\n",
      "         [0.7859, 0.4792, 0.2665]],\n",
      "\n",
      "        [[0.1918, 0.3231, 0.9610],\n",
      "         [0.7503, 0.5626, 0.2700]],\n",
      "\n",
      "        [[0.2092, 0.8292, 0.7639],\n",
      "         [0.6841, 0.6064, 0.9029]],\n",
      "\n",
      "        [[0.4516, 0.9106, 0.9012],\n",
      "         [0.2175, 0.6269, 0.4178]],\n",
      "\n",
      "        [[0.9009, 0.8265, 0.8337],\n",
      "         [0.9213, 0.0376, 0.2380]],\n",
      "\n",
      "        [[0.5426, 0.7731, 0.0437],\n",
      "         [0.2255, 0.9555, 0.9879]],\n",
      "\n",
      "        [[0.6404, 0.5305, 0.2738],\n",
      "         [0.4166, 0.6042, 0.3748]],\n",
      "\n",
      "        [[0.7394, 0.3000, 0.6479],\n",
      "         [0.0416, 0.6007, 0.4241]],\n",
      "\n",
      "        [[0.8982, 0.4961, 0.6603],\n",
      "         [0.4529, 0.2528, 0.1696]],\n",
      "\n",
      "        [[0.5100, 0.7112, 0.9630],\n",
      "         [0.6973, 0.5117, 0.5903]]])\n"
     ]
    }
   ],
   "source": [
    "random_tensor = torch.rand(size=(10, 2, 3))\n",
    "print(f'random_tensor =\\n {random_tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b225fd74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:53:58.226303Z",
     "start_time": "2025-05-04T22:53:58.224601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 3\n",
      "Size: torch.Size([10, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensions: {random_tensor.ndim}')\n",
    "print(f'Size: {random_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05029fa",
   "metadata": {},
   "source": [
    "- *We will represent images as three-dimensional tensors, where the first dimension represents the color channels (RGB), while the second and third dimensions represent the number of pixels in the image. For example, if we have a 1024x1024 color image and convert it to a tensor, then we would have a three-dimensional tensor with size `torch.Size([3, 1024, 1024])`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61226c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:55:00.861854Z",
     "start_time": "2025-05-04T22:55:00.841873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4814, 0.0632, 0.9753,  ..., 0.1883, 0.0756, 0.9011],\n",
       "         [0.3321, 0.1456, 0.4719,  ..., 0.8391, 0.7805, 0.9025],\n",
       "         [0.8482, 0.3054, 0.4833,  ..., 0.8161, 0.4669, 0.0689],\n",
       "         ...,\n",
       "         [0.6489, 0.1121, 0.1003,  ..., 0.7453, 0.2007, 0.9964],\n",
       "         [0.2765, 0.6053, 0.9880,  ..., 0.5661, 0.5327, 0.3379],\n",
       "         [0.1350, 0.9351, 0.9738,  ..., 0.9875, 0.6422, 0.8706]],\n",
       "\n",
       "        [[0.3541, 0.7974, 0.2714,  ..., 0.3374, 0.7037, 0.5539],\n",
       "         [0.7805, 0.4649, 0.8135,  ..., 0.0389, 0.6538, 0.3782],\n",
       "         [0.9135, 0.1864, 0.4188,  ..., 0.3658, 0.5919, 0.9976],\n",
       "         ...,\n",
       "         [0.8582, 0.4580, 0.7376,  ..., 0.6061, 0.8997, 0.4062],\n",
       "         [0.8784, 0.2916, 0.4300,  ..., 0.4109, 0.7739, 0.8302],\n",
       "         [0.6331, 0.1060, 0.0243,  ..., 0.3143, 0.0407, 0.0370]],\n",
       "\n",
       "        [[0.2824, 0.8100, 0.5089,  ..., 0.4428, 0.2192, 0.8499],\n",
       "         [0.1997, 0.4274, 0.6938,  ..., 0.2850, 0.7926, 0.9036],\n",
       "         [0.9079, 0.3626, 0.6193,  ..., 0.9576, 0.1364, 0.1978],\n",
       "         ...,\n",
       "         [0.1833, 0.9051, 0.0858,  ..., 0.2803, 0.3018, 0.6405],\n",
       "         [0.5114, 0.4080, 0.4862,  ..., 0.8726, 0.3103, 0.4125],\n",
       "         [0.8540, 0.4906, 0.4830,  ..., 0.5194, 0.8921, 0.8501]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image_tensor = torch.rand(size=(3, 1024, 1024))\n",
    "random_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f68d5df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:55:11.707707Z",
     "start_time": "2025-05-04T22:55:11.705950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones: 3\n",
      "Size: torch.Size([3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimensiones: {random_image_tensor.ndim}')\n",
    "print(f'Size: {random_image_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8b08e558b8e53",
   "metadata": {},
   "source": [
    "- *Whenever we work with random (or pseudo-random) processes, we will observe differences in the results. Even with the same seed, the same random process may generate different results if we run it on different devices (i.e., CPU or GPU). For more information about working with non-deterministic processes in PyTorch, read the following [documentation](https://pytorch.org/docs/stable/notes/randomness.html).*\n",
    "\n",
    "- *We can define a seed that allows us to reproduce the results of an experiment using the `torch.manual_seed()` function (the same seed is defined for all devices). Now, running the same code multiple times returns the same result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c9dd953aa1bcb2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:56:04.022442Z",
     "start_time": "2025-05-04T22:56:04.011146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor 1: tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904]])\n",
      "Random tensor 2: tensor([[0.6009, 0.2566, 0.7936, 0.9408, 0.1332]])\n",
      "Are the tensors equal? False\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random_tensor_1 = torch.rand(size=(1, 5))\n",
    "random_tensor_2 = torch.rand(size=(1, 5))\n",
    "\n",
    "print(f'Random tensor 1: {random_tensor_1}')\n",
    "print(f'Random tensor 2: {random_tensor_2}')\n",
    "\n",
    "# We can compare two tensors using the function torch.equal() or the operator ==\n",
    "print(f'Are the tensors equal? {torch.equal(random_tensor_1, random_tensor_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5c870d06425d7",
   "metadata": {},
   "source": [
    "- *Note that, despite using the same seed, the tensors we generate are not equal to each other. To get two equal tensors we would need to run `torch.manual_seed()` before creating each of the tensors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55962d6e86c71698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:56:40.933172Z",
     "start_time": "2025-05-04T22:56:40.927012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor 1: tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904]])\n",
      "Random tensor 2: tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904]])\n",
      "Are the tensors equal? True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) # Definimos la semilla para el primer tensor\n",
    "random_tensor_1 = torch.rand(size=(1, 5))\n",
    "torch.manual_seed(42) # Definimos la semilla para el segundo tensor\n",
    "random_tensor_2 = torch.rand(size=(1, 5))\n",
    "\n",
    "print(f'Random tensor 1: {random_tensor_1}')\n",
    "print(f'Random tensor 2: {random_tensor_2}')\n",
    "\n",
    "# We can compare two tensors using the function torch.equal() or the operator ==\n",
    "print(f'Are the tensors equal? {torch.equal(random_tensor_1, random_tensor_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f1a17",
   "metadata": {},
   "source": [
    "## Tensor-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a5c4383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:56:50.242604Z",
     "start_time": "2025-05-04T22:56:50.236978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  36,  72, 108, 144, 180, 216, 252, 288, 324, 360, 396, 432, 468,\n",
       "        504, 540, 576, 612, 648, 684, 720, 756, 792, 828, 864, 900, 936, 972])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_tensor = torch.arange(start=0, end=1000, step=36)\n",
    "range_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d6d2dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:56:52.963045Z",
     "start_time": "2025-05-04T22:56:52.960151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1\n",
      "Shape: torch.Size([28])\n",
      "Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimension: {range_tensor.ndim}')\n",
    "print(f'Shape: {range_tensor.shape}')\n",
    "print(f'Dtype: {range_tensor.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fc86a",
   "metadata": {},
   "source": [
    "- *When we say we're going to create or that we created an object that is \"tensor-like\", we refer to creating a new tensor that has the same dimension, size, and data type as another existing tensor, but with different values.*\n",
    "\n",
    "- *For example, we can create a new tensor from the `range_tensor` tensor that has the same dimension, size, and data type, but contains different information (only zeros).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f911467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:57:22.741123Z",
     "start_time": "2025-05-04T22:57:22.735748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros_like(input=range_tensor)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b77d1008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:57:24.779021Z",
     "start_time": "2025-05-04T22:57:24.776212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1\n",
      "Shape: torch.Size([28])\n",
      "Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Dimension: {zeros.ndim}')\n",
    "print(f'Shape: {zeros.shape}')\n",
    "print(f'Dtype: {zeros.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33743e580e9500f6",
   "metadata": {},
   "source": [
    "## Tensor attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316776f6719ea1a",
   "metadata": {},
   "source": [
    "### Data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67031bba",
   "metadata": {},
   "source": [
    "- *By default, PyTorch creates tensors using the `torch.float32` data type. There are many more data types [available](https://pytorch.org/docs/stable/tensors.html#data-types), which differ mainly in the numerical precision they provide. For example, the `torch.float64` data type has much more numerical precision than the `torch.float32` data type (almost double), but consumes much more memory.*\n",
    "\n",
    "- *We can specify the data type we want for our tensor using the `dtype` argument.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfbd0c5152e47f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:58:57.076837Z",
     "start_time": "2025-05-04T22:58:57.072572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_0 = torch.tensor([3., 2., 1.], dtype=torch.float64)\n",
    "TENSOR_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45843e9f473bafb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T22:58:59.700206Z",
     "start_time": "2025-05-04T22:58:59.696680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_0.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207eed92c6d856b6",
   "metadata": {},
   "source": [
    "- *We can modify the data type using the `torch.Tensor.type()` method or the `torch.Tensor.to()` method. The latter method is the most recommended, as it also allows us to modify other tensor attributes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a35ed321e5f5eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:05:07.778888Z",
     "start_time": "2025-05-04T23:05:07.774382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 1.], dtype=torch.float16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change tensor's datatype\n",
    "TENSOR_1 = TENSOR_0.to(dtype=torch.float16) # or use torch.Tensor.to()\n",
    "TENSOR_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea516daba2733c9",
   "metadata": {},
   "source": [
    "- *When performing mathematical operations between two tensors with different data types, PyTorch uses a **promotion rule** to define the data type of the resulting tensor (it will have the data type with higher precision). For example, if we multiply a tensor with data type `torch.float32` and another with data type `torch.float64`, the tensor we get after the operation is `torch.float64`.*\n",
    "\n",
    "- *This is something important to keep in mind because:*\n",
    "    1. *We can end up consuming more memory than expected if the operations we perform are constantly increasing the numerical precision of our tensors.*\n",
    "    2. *Operations between tensors with different data types are usually less performant because PyTorch also has to handle data conversion.*\n",
    "    3. *Some devices have limited support for some data types (e.g., GPUs don't support tensors with `torch.float64` data types or `mps` doesn't support tensors with `torch.float16` data types).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "125494bc90c63117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:05:08.936692Z",
     "start_time": "2025-05-04T23:05:08.931977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 4., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR_0 * TENSOR_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba96f0fa9bfdbf",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e63427",
   "metadata": {},
   "source": [
    "- *We can also define the **device** where the tensor is stored and executed. There are several types of devices we can use, but the most commonly used are:*\n",
    "    1. *`cpu`: allows us to store and execute tensors in our computer's or virtual machine's RAM.*\n",
    "    2. *`cuda`: allows us to store and execute tensors on the GPU. If we have more than one GPU available, we can specify which one we want to use, e.g., `cuda:0` (first GPU), `cuda:1` (second GPU), `cuda:2` (third GPU), etc.*\n",
    "    3. *`mps`: allows us to use the integrated GPU in computers with Apple Silicon (similar to how `cuda` allows us to use NVIDIA GPUs). For more information about using MPS with PyTorch, read the following [documentation](https://pytorch.org/docs/stable/notes/mps.html).*\n",
    "\n",
    "- *Why is it important to define the device?:*\n",
    "    1. *If we have two tensors that are stored on two different devices, we cannot perform operations between them.*\n",
    "    2. *Moving tensors from one device to another can be computationally expensive.*\n",
    "    3. *The data type we have available to use depends on the device we use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65c0aaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:08:51.728856Z",
     "start_time": "2025-05-04T23:08:51.723629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae0318",
   "metadata": {},
   "source": [
    "- *We can choose the device where we want to store and execute the tensor at the time of creation:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6e619b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:09:26.883761Z",
     "start_time": "2025-05-04T23:09:26.874750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([[[0.2137, 0.1287, 0.7578,  ..., 0.5280, 0.1456, 0.7621],\n",
      "         [0.8253, 0.4947, 0.8110,  ..., 0.7116, 0.7734, 0.5908],\n",
      "         [0.3095, 0.3278, 0.5857,  ..., 0.7197, 0.9459, 0.6801],\n",
      "         ...,\n",
      "         [0.8766, 0.0069, 0.1601,  ..., 0.7396, 0.1457, 0.8224],\n",
      "         [0.9497, 0.4182, 0.1513,  ..., 0.2655, 0.8717, 0.3968],\n",
      "         [0.3618, 0.1220, 0.6681,  ..., 0.9665, 0.2104, 0.3793]],\n",
      "\n",
      "        [[0.5913, 0.7940, 0.0250,  ..., 0.3615, 0.9619, 0.2593],\n",
      "         [0.3742, 0.9553, 0.4215,  ..., 0.9176, 0.5058, 0.0018],\n",
      "         [0.3236, 0.1479, 0.7749,  ..., 0.4080, 0.7298, 0.5154],\n",
      "         ...,\n",
      "         [0.1911, 0.9423, 0.8503,  ..., 0.5304, 0.1596, 0.3789],\n",
      "         [0.2172, 0.7798, 0.9750,  ..., 0.1987, 0.9614, 0.8083],\n",
      "         [0.8856, 0.7259, 0.3923,  ..., 0.7049, 0.5056, 0.8305]],\n",
      "\n",
      "        [[0.7707, 0.6171, 0.6775,  ..., 0.1210, 0.4782, 0.9287],\n",
      "         [0.0903, 0.1733, 0.3913,  ..., 0.9720, 0.0357, 0.6195],\n",
      "         [0.6506, 0.2457, 0.7716,  ..., 0.9107, 0.5576, 0.1392],\n",
      "         ...,\n",
      "         [0.9482, 0.6730, 0.3985,  ..., 0.8227, 0.8427, 0.0947],\n",
      "         [0.3608, 0.9684, 0.7672,  ..., 0.7445, 0.2831, 0.8522],\n",
      "         [0.2800, 0.0937, 0.7143,  ..., 0.1224, 0.1450, 0.0304]]])\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define a torch.device\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a new tensor with specified device\n",
    "TENSOR_2 = torch.rand(size=(3, 1024, 1024), device=torch_device)\n",
    "print(f'Tensor: {TENSOR_2}')\n",
    "print(f'Device: {TENSOR_2.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d709",
   "metadata": {},
   "source": [
    "- *We can also use the `torch.Tensor.to` function to change the data type or device of a tensor (note that it's necessary to reassign the tensor to the variable):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6d77296c9425c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:09:41.631193Z",
     "start_time": "2025-05-04T23:09:41.429516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps:0\n"
     ]
    }
   ],
   "source": [
    "TENSOR_2 = TENSOR_2.to(device='mps')\n",
    "print(f'Device: {TENSOR_2.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a232e5b62633ef3",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bdc51199bf470",
   "metadata": {},
   "source": [
    "- *Another important argument for tensors is the `requires_grad` argument. When we define `requires_grad=True` in a tensor, we're asking PyTorch to save all operations performed on that tensor, so we can calculate gradients. This is very useful when we want to train a Deep Learning model, as it allows us to calculate the gradients of the loss function with respect to the model weights using backpropagation.*\n",
    "\n",
    "- *Note that in the following example we create a new variable, `y`, which equals the sum of the elements in `X`. Then, to calculate the gradients of `y` with respect to the elements in `X` (i.e., how the value of `y` changes with an infinitesimal change in any element of `X`), we call the `.backward()` function from variable `y`. However, we can find the gradients in tensor `X` (this makes sense because the gradients tell us how `y` changes when an element of `X` changes).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d82fb567c1721aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:10:48.940725Z",
     "start_time": "2025-05-04T23:10:48.773020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients after automatic differentiation: tensor([1., 1., 1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Creates a tensor and operates on it\n",
    "X = torch.tensor([3., 2., 1.], requires_grad=True, device='mps')\n",
    "\n",
    "# Sum all elements of the tensor and compute the gradients\n",
    "y = X.sum()\n",
    "y.backward()\n",
    "print(f'Gradients after automatic differentiation: {X.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994917e34f947c57",
   "metadata": {},
   "source": [
    "## PyTorch and NumPy\n",
    "\n",
    "- *We can create tensors from NumPy arrays, and vice versa. This is very useful because we can use NumPy functionalities to manipulate our data, and then use PyTorch tensors to train our Deep Learning models.*\n",
    "\n",
    "- *We can create a tensor from a NumPy array using the `torch.from_numpy(ndarray)` function. If we want to convert a tensor to a NumPy array, we have to use the `torch.Tensor.numpy()` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9197359ed8de27f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:13:12.114081Z",
     "start_time": "2025-05-04T23:13:12.111410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array de NumPy: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(1., 10.).reshape((3, 3))\n",
    "print(f'Array de NumPy: \\n{array}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e337249c29cd1375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:13:12.746984Z",
     "start_time": "2025-05-04T23:13:12.743206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor de PyTorch: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.from_numpy(array)\n",
    "print(f'Tensor de PyTorch: \\n{tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c0471e2713fff",
   "metadata": {},
   "source": [
    "- *Something very important to keep in mind is that the default data type for NumPy is different from PyTorch's (NumPy uses `float64` while PyTorch uses `float32`). The tensor we create from the NumPy array inherits the array's data type, which could cause problems when trying to operate between tensors (if the data types don't match).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f2bdc051f29607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:13:15.243594Z",
     "start_time": "2025-05-04T23:13:15.241172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array's datatype: float64\n",
      "Tensor's datatype: torch.float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Array's datatype: {array.dtype}\")\n",
    "print(f\"Tensor's datatype: {tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f8e4f7ea8a36f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:13:16.717111Z",
     "start_time": "2025-05-04T23:13:16.714578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor's datatype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "tensor = tensor.to(dtype=torch.float32)\n",
    "print(f\"Tensor's datatype: {tensor.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
