{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import torch",
   "id": "c0ae4ab6cf7d5983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modificar tensores\n",
    "\n",
    "1. *Vista (`torch.Tensor.view`). Nos permite cambiar la forma del tensor, similar a lo que hace el método `torch.Tensor.reshape()`, pero con dos grandes diferencias:*\n",
    "    - *No hace una copia del tensor en memoria. El nuevo tensor referencia al mismo objeto en memoria.*\n",
    "    - *Solo funciona si el tensor es **contiguo** en memoria.*\n",
    "        - *Que un tensor sea contiguo quiere decir que sus elementos están guardados de forma secuencial en un bloque de memoria ininterrumpido (vector uni-dimensional).*\n",
    "    - *Hay operaciones que pueden hacer que un tensor no sea contiguo, por ejemplo, la transposición de un tensor. En este caso, este método nos devolverá un error (se puede llamar al método `torch.Tensor.contiguous()` antes de llamar a `torch.Tensor.view()`).*\n",
    "2. *Reshape (`torch.Tensor.reshape()`). Nos permite cambiar la forma del tensor sin cambiar sus elementos. Por ejemplo, cambia la dimensión de $3\\times 2$ a $2\\times 3$.*\n",
    "    - *No requiere que el tensor sea contiguo en memoria. Si el tensor es contiguo, llama al método `torch.Tensor.view()`. Si no es contiguo, entonces primero llama al método `torch.Tensor.contiguous()`, creando una copia del tensor en memoria que sí es contigua.*\n",
    "    - *Si no estamos seguros de si el tensor es contiguo o no, es mejor usar `torch.Tensor.reshape()` en lugar de `torch.Tensor.view()`.*\n",
    "3. *Concatenar. Hay varias opciones disponibles para concatenar tensores:*\n",
    "    -  *`torch.cat()`. Permite concatenar tensores sobre una dimensión existente, resultando en un tensor con la misma cantidad de dimensiones. Por ejemplo, si tenemos dos tensor de $2\\times 2$, y los concatenamos usando `dim=0` (en este caso filas), el resultado será un tensor de $4\\times 2$, mientras que si los concatenamos usando `dim=1` (en este caso columnas), el resultado será un tensor de $2\\times 4$.*\n",
    "        - *No es necesario que la dimensión sobre la que concatenamos tenga el mismo tamaño, pero las dimensiones restantes deben ser iguales.*\n",
    "    - *`torch.stack()`. Permite concatenar tensores sobre una nueva dimensión, resultando en un tensor con una dimensión adicional. Por ejemplo, si tenemos dos tensor de $2\\times 2$, y los concatenamos usando `dim=0`, entonces nos devuelve un tensor de dimensión $2\\times2\\times2$.*\n",
    "        - *Ambos tensores tienen que tener el mismo tamaño.*\n",
    "4. *Eliminar/agregar dimensions. La función `torch.squeezed()` nos permite eliminar dimensiones de tamaño 1, mientras que la función `torch.unsqueezed()` nos permite agregar dimensiones de tamaño 1.*\n",
    "5. *Permutar dimensiones. Con la función `torch.permuted()` podemos re-acomodar las dimensiones de un tensor.*"
   ],
   "id": "3cac9aa4b767f9c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vistas",
   "id": "f78c8086675c2bab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:48:56.298999Z",
     "start_time": "2025-04-20T15:48:56.294123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.arange(0, 10)\n",
    "X = X.type(torch.float32)\n",
    "X"
   ],
   "id": "9f0d33a832b3d459",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:48:57.488619Z",
     "start_time": "2025-04-20T15:48:57.484642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = X.view(2, 5)\n",
    "y"
   ],
   "id": "b6fc414ac59ebc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Creamos un nuevo tensor `y`, que es una vista del tensor `X`, de tamaño $2\\times 5$. Ahora, cambiar uno de los elementos del tensor `y` afectaría al tensor `X`? Sí, porque `y` hace referencia al mismo objeto en memoria que el tensor `X`, por lo tanto, si cambiamos `y` también vamos a cambiar a `X`.*",
   "id": "6ec0b3489df502e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:49:00.652657Z",
     "start_time": "2025-04-20T15:49:00.647013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y[:, 0] = torch.tensor([10, 20], dtype=torch.float32)\n",
    "y"
   ],
   "id": "1d06f9b5d11278fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  1.,  2.,  3.,  4.],\n",
       "        [20.,  6.,  7.,  8.,  9.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:49:01.753402Z",
     "start_time": "2025-04-20T15:49:01.748975Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "f3df138aae01fd74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  1.,  2.,  3.,  4., 20.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reshape",
   "id": "d6d4145fb80855db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:54:36.826899Z",
     "start_time": "2025-04-20T15:54:36.823841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.rand(size=(2, 3))\n",
    "X"
   ],
   "id": "5d802423036176af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3700, 0.5732, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:54:46.110464Z",
     "start_time": "2025-04-20T15:54:46.106634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = X.reshape(3, -1) # -1 means that the size of that dimension is defined automatically\n",
    "y"
   ],
   "id": "8cacdb245f20cfee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3700, 0.5732],\n",
       "        [0.2554, 0.2901],\n",
       "        [0.9397, 0.7976]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Podemos ver que en este caso el método `reshape` nos devuelve una vista del tensor `X` porque un cambio en `y` impacta en el tensor `X`. Esto porque el tensor `X` sigue siendo contiguo en memoria.*",
   "id": "ed035d53c63de10d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:55:39.113841Z",
     "start_time": "2025-04-20T15:55:39.109269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y[0, :] = torch.tensor([1, 1])\n",
    "y"
   ],
   "id": "8c9ffb13cfe009d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000],\n",
       "        [0.2554, 0.2901],\n",
       "        [0.9397, 0.7976]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:55:43.052766Z",
     "start_time": "2025-04-20T15:55:43.048381Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "ecd7a01a37006426",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Creamos un nuevo tensor, `z`, que es la versión transpuesta de `X`, y a partir de ese nuevo tensor creamos el tensor `y`. Ahora, si realizamos un cambio en `y`, este cambio no va a impactar en `X`. Por qué ocurre esto? Porque al crear el tensor `z` a partir de la versión transpuesta de `X`, ese tensor deja de ser contiguo en memoria, y entonces el método `reshape()` crea una copia de `z` y no una vista.*",
   "id": "8ddd97b4313189a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:22.326646Z",
     "start_time": "2025-04-20T15:56:22.322281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = X.T\n",
    "z"
   ],
   "id": "a0e1414dcf99e3a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2901],\n",
       "        [1.0000, 0.9397],\n",
       "        [0.2554, 0.7976]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:31.390496Z",
     "start_time": "2025-04-20T15:56:31.385737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = z.reshape(2, -1)\n",
    "y"
   ],
   "id": "c299133b9ec7d18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2901, 1.0000],\n",
       "        [0.9397, 0.2554, 0.7976]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:56:55.731344Z",
     "start_time": "2025-04-20T15:56:55.726587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y[:, 0] = torch.tensor([0, 0])\n",
    "y"
   ],
   "id": "4639516002c4b2ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2901, 1.0000],\n",
       "        [0.0000, 0.2554, 0.7976]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T15:59:26.492420Z",
     "start_time": "2025-04-20T15:59:26.488714Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "ca8587f737bd81c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2554],\n",
       "        [0.2901, 0.9397, 0.7976]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Concatenar",
   "id": "b1212f80c4dc96f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:45:54.029629Z",
     "start_time": "2025-04-20T17:45:54.020957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_A = torch.rand(size=(2, 3))\n",
    "tensor_B = torch.rand(size=(2, 3))\n",
    "\n",
    "print(f'tensor_A:\\n {tensor_A}')\n",
    "print(f'tensor_B:\\n {tensor_B}')"
   ],
   "id": "8d156f89c95392ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_A:\n",
      " tensor([[0.9063, 0.8582, 0.9965],\n",
      "        [0.8017, 0.4278, 0.9973]])\n",
      "tensor_B:\n",
      " tensor([[0.9701, 0.1752, 0.0940],\n",
      "        [0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Si usamos `torch.Tensor.cat()`, podemos concatenar dos tensores sobre una dimensión existente (mantiene el tamaño de las dimensiones sobre las que no concatenamos). Por ejemplo, si concatenamos sobre la `dim=0`, nos queda un tensor de $4\\times 2$ (se mantiene el tamaño de la segunda dimensión).*",
   "id": "64b2dff0dc185717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:47:17.123423Z",
     "start_time": "2025-04-20T17:47:17.119979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_C = torch.cat((tensor_A, tensor_B), dim=0)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ],
   "id": "3db4c5ae034788bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([4, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9063, 0.8582, 0.9965],\n",
      "        [0.8017, 0.4278, 0.9973],\n",
      "        [0.9701, 0.1752, 0.0940],\n",
      "        [0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Si, por otro lado, usamos la dimensión `dim=1`, nos queda un tensor de $2\\times6$.*",
   "id": "9618afb8c9a8340a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:49:10.010820Z",
     "start_time": "2025-04-20T17:49:10.006694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_C = torch.cat((tensor_A, tensor_B), dim=1)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ],
   "id": "fe950f633e4165af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([2, 6])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9063, 0.8582, 0.9965, 0.9701, 0.1752, 0.0940],\n",
      "        [0.8017, 0.4278, 0.9973, 0.1659, 0.8378, 0.3756]])\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Si usamos `torch.Tensor.stack()`, creamos una nueva dimensión. Entonces, en este caso nos queda un tensor de tres dimensiones a partir de concatenar dos tensores de dos dimensiones. En este caso es necesario que ambos tensores tengan las mismas dimensiones.*",
   "id": "cd05e1dfd094996a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T17:53:17.158865Z",
     "start_time": "2025-04-20T17:53:17.153760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_C = torch.stack((tensor_A, tensor_B), dim=0)\n",
    "print(f'Original shape: tensor_A = {tensor_A.shape} and tensor_B = {tensor_B.shape}')\n",
    "print(f'Shape after concatenation: tensor_C = {tensor_C.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_C}')"
   ],
   "id": "47ecfd256f4b4a23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: tensor_A = torch.Size([2, 3]) and tensor_B = torch.Size([2, 3])\n",
      "Shape after concatenation: tensor_C = torch.Size([2, 2, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9063, 0.8582, 0.9965],\n",
      "         [0.8017, 0.4278, 0.9973]],\n",
      "\n",
      "        [[0.9701, 0.1752, 0.0940],\n",
      "         [0.1659, 0.8378, 0.3756]]])\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Eliminar/agregar dimensiones",
   "id": "4e172b9cb909a2ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*La función `torch.squeezed()` nos permite eliminar dimensiones de tamaño 1 de un tensor. El argumento `dim` nos permite elegir que dimensiones queremos eliminar.*\n",
    "- *Hay que tener en cuenta que el tensor resultante de esta función es una vista (`torch.Tensor.view`), lo que quiere decir que comparte el mismo objeto en memoria que el tensor original.*"
   ],
   "id": "bc295a5426e0f142"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:19:24.617166Z",
     "start_time": "2025-04-20T18:19:24.613618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.rand(size=(1, 2, 3))\n",
    "X"
   ],
   "id": "3215589b47dc74d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1866, 0.0648, 0.6463],\n",
       "         [0.6808, 0.5452, 0.9616]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Quitamos la primer dimensión que tiene tamaño 1. El tensor resultante, `tensor_X_squeezed`, es una matriz de $2\\times 3$.*",
   "id": "d9caa8cf1694332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:13:44.486391Z",
     "start_time": "2025-04-20T18:13:44.483407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_X_squeezed = torch.squeeze(X)\n",
    "print(f'Original shape: {X.shape}')\n",
    "print(f'Shape after squeezing: {tensor_X_squeezed.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_X_squeezed}')"
   ],
   "id": "ba256914e05d08bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 2, 3])\n",
      "Shape after squeezing: torch.Size([2, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[0.9786, 0.7882, 0.6897],\n",
      "        [0.1621, 0.8248, 0.8890]])\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Por otro lado, la función `torch.unsqueezed()` nos permite agregar una dimensión (debemos especificarla). En el código debajo volvemos a agregar la dimensión que quitamos anteriormente.*",
   "id": "1ce183fcbb3349c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:17:43.788727Z",
     "start_time": "2025-04-20T18:17:43.786137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_X_unsqueezed = torch.unsqueeze(tensor_X_squeezed, dim=0)\n",
    "print(f'Shape squeezed tensor: {tensor_X_squeezed.shape}')\n",
    "print(f'Shape after unsqueezing: {tensor_X_unsqueezed.shape}\\n')\n",
    "print(f'New tensor:\\n {tensor_X_unsqueezed}')"
   ],
   "id": "98cae64360ee19ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape squeezed tensor: torch.Size([2, 3])\n",
      "Shape after unsqueezing: torch.Size([2, 1, 3])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9786, 0.7882, 0.6897]],\n",
      "\n",
      "        [[0.1621, 0.8248, 0.8890]]])\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Permutar dimensiones\n",
    "\n",
    "*La función `torch.permuted()` nos permite re-acomodar las dimensiones de un tensor. Por ejemplo, tenemos un tensor de $1024\\times1024\\times3$ (podemos pensar que es una imagen, donde las dos primeras dimensiones hacen referencia a la altura y el largo, y la última dimensión hace referencia a los canales de colores -RGB-). Podríamos cambiar la última dimensión para que sea la primera (i.e., `dim=0`).*"
   ],
   "id": "d2f72629bd7e74e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:31:54.669623Z",
     "start_time": "2025-04-20T18:31:54.654483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = torch.rand(size=(1024, 1024, 3))\n",
    "image"
   ],
   "id": "8e7b0a8cb115f1ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9683, 0.9730, 0.2627],\n",
       "         [0.8097, 0.8765, 0.0550],\n",
       "         [0.6498, 0.4069, 0.7136],\n",
       "         ...,\n",
       "         [0.3865, 0.8466, 0.9223],\n",
       "         [0.7619, 0.6037, 0.4737],\n",
       "         [0.6288, 0.7732, 0.5016]],\n",
       "\n",
       "        [[0.7348, 0.9181, 0.7637],\n",
       "         [0.3168, 0.6106, 0.4738],\n",
       "         [0.4297, 0.4724, 0.0670],\n",
       "         ...,\n",
       "         [0.6849, 0.3207, 0.5561],\n",
       "         [0.5420, 0.3546, 0.4909],\n",
       "         [0.5327, 0.2005, 0.7087]],\n",
       "\n",
       "        [[0.5963, 0.4694, 0.3344],\n",
       "         [0.8822, 0.2125, 0.4717],\n",
       "         [0.0236, 0.4680, 0.9261],\n",
       "         ...,\n",
       "         [0.5447, 0.1777, 0.9908],\n",
       "         [0.6046, 0.7683, 0.6753],\n",
       "         [0.4742, 0.0015, 0.0772]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8567, 0.7679, 0.7789],\n",
       "         [0.4698, 0.0566, 0.6014],\n",
       "         [0.8258, 0.4664, 0.5827],\n",
       "         ...,\n",
       "         [0.7187, 0.1043, 0.7711],\n",
       "         [0.3493, 0.7985, 0.1038],\n",
       "         [0.0346, 0.3198, 0.4269]],\n",
       "\n",
       "        [[0.6278, 0.5091, 0.4229],\n",
       "         [0.0168, 0.8149, 0.7544],\n",
       "         [0.8417, 0.5538, 0.8600],\n",
       "         ...,\n",
       "         [0.7759, 0.1778, 0.3747],\n",
       "         [0.0819, 0.9297, 0.4055],\n",
       "         [0.1806, 0.2597, 0.5550]],\n",
       "\n",
       "        [[0.7465, 0.3158, 0.8514],\n",
       "         [0.6822, 0.2986, 0.1693],\n",
       "         [0.9011, 0.9336, 0.3586],\n",
       "         ...,\n",
       "         [0.5210, 0.7761, 0.0670],\n",
       "         [0.7169, 0.6780, 0.8586],\n",
       "         [0.2203, 0.3433, 0.3857]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Para permutar las dimensiones del tensor, tenemos que especificar las dimensiones del tensor actual que queremos en la dimensión del nuevo tensor. En este caso, queremos que el nuevo orden sea: la tercera dimensión primero (i.e., `dim=2`), después la primer dimensión (i.e., `dim=0`) y finalmente la segunda dimensión (i.e., `dim=1`).*",
   "id": "40b4b8d5bb98644e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T18:36:53.786406Z",
     "start_time": "2025-04-20T18:36:53.781308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_permuted = torch.permute(image, dims=(2, 0, 1))\n",
    "print(f'Original shape: {image.shape}')\n",
    "print(f'Shape after permuting: {image_permuted.shape}\\n')\n",
    "print(f'New tensor:\\n {image_permuted}')"
   ],
   "id": "3da04e1614a5898b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1024, 1024, 3])\n",
      "Shape after permuting: torch.Size([3, 1024, 1024])\n",
      "\n",
      "New tensor:\n",
      " tensor([[[0.9683, 0.8097, 0.6498,  ..., 0.3865, 0.7619, 0.6288],\n",
      "         [0.7348, 0.3168, 0.4297,  ..., 0.6849, 0.5420, 0.5327],\n",
      "         [0.5963, 0.8822, 0.0236,  ..., 0.5447, 0.6046, 0.4742],\n",
      "         ...,\n",
      "         [0.8567, 0.4698, 0.8258,  ..., 0.7187, 0.3493, 0.0346],\n",
      "         [0.6278, 0.0168, 0.8417,  ..., 0.7759, 0.0819, 0.1806],\n",
      "         [0.7465, 0.6822, 0.9011,  ..., 0.5210, 0.7169, 0.2203]],\n",
      "\n",
      "        [[0.9730, 0.8765, 0.4069,  ..., 0.8466, 0.6037, 0.7732],\n",
      "         [0.9181, 0.6106, 0.4724,  ..., 0.3207, 0.3546, 0.2005],\n",
      "         [0.4694, 0.2125, 0.4680,  ..., 0.1777, 0.7683, 0.0015],\n",
      "         ...,\n",
      "         [0.7679, 0.0566, 0.4664,  ..., 0.1043, 0.7985, 0.3198],\n",
      "         [0.5091, 0.8149, 0.5538,  ..., 0.1778, 0.9297, 0.2597],\n",
      "         [0.3158, 0.2986, 0.9336,  ..., 0.7761, 0.6780, 0.3433]],\n",
      "\n",
      "        [[0.2627, 0.0550, 0.7136,  ..., 0.9223, 0.4737, 0.5016],\n",
      "         [0.7637, 0.4738, 0.0670,  ..., 0.5561, 0.4909, 0.7087],\n",
      "         [0.3344, 0.4717, 0.9261,  ..., 0.9908, 0.6753, 0.0772],\n",
      "         ...,\n",
      "         [0.7789, 0.6014, 0.5827,  ..., 0.7711, 0.1038, 0.4269],\n",
      "         [0.4229, 0.7544, 0.8600,  ..., 0.3747, 0.4055, 0.5550],\n",
      "         [0.8514, 0.1693, 0.3586,  ..., 0.0670, 0.8586, 0.3857]]])\n"
     ]
    }
   ],
   "execution_count": 95
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
